{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consolidated Active Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consolidated script for running entire active learning finetuning pipeline\n",
    "# This script is meant to be run on a single GPU, evaluated on Google Colab\n",
    "# Datasets are requested from https://huggingface.co/datasets/dxiao/requirements-ner-id\n",
    "# Final finetuned model can be accessed from https://huggingface.co/dxiao/bert-finetuned-ner-100percent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cloud install for Colab\n",
    "!pip install datasets\n",
    "!pip install transformers\n",
    "!pip install evaluate\n",
    "!pip install seqeval\n",
    "!pip install numba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In case model_upload in preferred: Login into huggingface_hub\n",
    "# Token: *insert own huggingface token*\n",
    "# from huggingface_hub import notebook_login\n",
    "# notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# HuggingFace packages\n",
    "from datasets import load_dataset\n",
    "from datasets import concatenate_datasets\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import Trainer\n",
    "from transformers import TrainingArguments\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "from transformers import AutoModelForTokenClassification\n",
    "import evaluate\n",
    "from evaluate import evaluator\n",
    "from tqdm.notebook import trange\n",
    "\n",
    "import torch\n",
    "from numba import jit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "STEP_PERCENT = 2 # Step-size with each step as % of the training dataset\n",
    "TOTAL_SIZE = 600 # total size of training dataset\n",
    "SEED = 0 # chosen seed for reproducibility\n",
    "DATASET_CONFIG = \"dxiao/requirements-ner-id\" # dataset from huggingface\n",
    "MODEL_CONFIG = \"bert-base-cased\" # pre-trained model for further finetuning\n",
    "AL_STRATEGY = 'MA'  # selected query strategy. \n",
    "                    # Select from \"LC\", \"RS\" or \"MA\" or \"EN\"\n",
    "                    # \"RS\" = random sampling\n",
    "                    # \"LC\" = least confidence\n",
    "                    # \"MA\" = margin sampling\n",
    "                    # \"EN\" = entropy sampling\n",
    "\n",
    "# parameter calculations\n",
    "iterations = int(100/STEP_PERCENT)\n",
    "step_size = int(TOTAL_SIZE/iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration dxiao--requirements-ner-a9d27206730c3bd0\n",
      "Found cached dataset json (C:/Users/dekai/.cache/huggingface/datasets/dxiao___json/dxiao--requirements-ner-a9d27206730c3bd0/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44b76e33cb5a45aeadf62575d8c53d24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# import from huggingface\n",
    "raw_datasets = load_dataset(DATASET_CONFIG)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get label names\n",
    "ner_feature = raw_datasets[\"train\"].features[\"ner_tags\"] \n",
    "label_names = ner_feature.feature.names"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alignment function\n",
    "# special tokens = -100\n",
    "# replace tokens with have been split from B into B + I\n",
    "\n",
    "def align_labels_with_tokens(labels, word_ids):\n",
    "    new_labels = []\n",
    "    current_word_id = None\n",
    "    for word_id in word_ids:\n",
    "        if word_id != current_word_id: # check if it is not same word, just split into two\n",
    "            current_word_id = word_id \n",
    "            label = -100 if word_id is None else labels[word_id]\n",
    "            new_labels.append(label)\n",
    "        elif word_id is None:\n",
    "            # Special token\n",
    "            new_labels.append(-100)\n",
    "        else:\n",
    "            # Same word as previous token\n",
    "            label = labels[word_id]\n",
    "            # If the label is B-XXX we change it to I-XXX\n",
    "            if label % 2 == 1:\n",
    "                label += 1\n",
    "            new_labels.append(label)\n",
    "\n",
    "    return new_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaling align_labels_with_tokens function towards entire datasets by using lists in lists [[]]\n",
    "\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True) # similar to word_ids\n",
    "\n",
    "    all_labels = examples[\"ner_tags\"] # similar to labels but recursive [[ner_tag1],[ner_tag2],[ner_tag3]]\n",
    "    new_labels = []\n",
    "    for i, labels in enumerate(all_labels): # i = iterative number, labels = [ner_tag1]\n",
    "        word_ids = tokenized_inputs.word_ids(i) # takes word_ids from tokenizer magic based on iterative number \n",
    "        new_labels.append(align_labels_with_tokens(labels, word_ids)) # creates [[new_labels1],[new_labels2],[new_labels3]]\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = new_labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batched mapping function -> combines tokenize_and_align_labels() and align_labels_with_tokens()\n",
    "\n",
    "def batched_mapping(input_dataset):\n",
    "    output_dataset = input_dataset.map(\n",
    "    tokenize_and_align_labels,\n",
    "    batched=True,\n",
    "    # remove_columns=raw_datasets['train'].column_names, # keep columns \n",
    "    )\n",
    "    return output_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:/Users/dekai/.cache/huggingface/datasets/dxiao___json/dxiao--requirements-ner-a9d27206730c3bd0/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab\\cache-5992c15d856b607f.arrow\n",
      "Loading cached processed dataset at C:/Users/dekai/.cache/huggingface/datasets/dxiao___json/dxiao--requirements-ner-a9d27206730c3bd0/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab\\cache-e3b73f63a5239f7b.arrow\n",
      "Loading cached processed dataset at C:/Users/dekai/.cache/huggingface/datasets/dxiao___json/dxiao--requirements-ner-a9d27206730c3bd0/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab\\cache-d01a31e78a4352eb.arrow\n",
      "Loading cached shuffled indices for dataset at C:/Users/dekai/.cache/huggingface/datasets/dxiao___json/dxiao--requirements-ner-a9d27206730c3bd0/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab\\cache-5e0c786d550a9e87.arrow\n",
      "Loading cached shuffled indices for dataset at C:/Users/dekai/.cache/huggingface/datasets/dxiao___json/dxiao--requirements-ner-a9d27206730c3bd0/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab\\cache-9a78163c28ddc07f.arrow\n",
      "Loading cached shuffled indices for dataset at C:/Users/dekai/.cache/huggingface/datasets/dxiao___json/dxiao--requirements-ner-a9d27206730c3bd0/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab\\cache-ef91a4550da42e22.arrow\n"
     ]
    }
   ],
   "source": [
    "# apply batched mapping on train, test and validation\n",
    "\n",
    "tokenized_datasets = batched_mapping(raw_datasets)\n",
    "tokenized_datasets = tokenized_datasets.shuffle(seed=SEED)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query strategies"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Sampling (RS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slicing dataset into chunks\n",
    "\n",
    "sliced_datasets = {}\n",
    "\n",
    "for i in range(iterations): \n",
    "    sliced_datasets[i] = tokenized_datasets['train'].select(range(0,(i+1)*step_size)) # sliced_datasets[0] = first 10%\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inference Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generalizable inference\n",
    "# Input: dataset + inference_model -> Utilizes inference function\n",
    "# Output: df (incl. predictions)\n",
    "\n",
    "def general_inference(dataset, inference_model):\n",
    "    df = dataset.to_pandas()\n",
    "    df['predictions'] = df['tokens'].apply(lambda x: inference(x.tolist(),inference_model))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform inference on sentences\n",
    "# Input: word-tokenized sentence as list + model\n",
    "# Output: predictions [#words, #labels] as np.array\n",
    "\n",
    "def inference(sentence, inference_model):\n",
    "    encoding = tokenizer(sentence, return_tensors=\"pt\", truncation=True, is_split_into_words=True) #same params as in evalrun\n",
    "    outputs = inference_model(**encoding)\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.nn.functional.softmax(logits, dim=-1)[0]\n",
    "    predictions = predictions.detach().numpy()\n",
    "    return predictions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Least Confidence (LC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Least confidence strategy to obtain candidates\n",
    "# Input: df (incl. predictions) + step_size -> Utilizes least_confidence_calculation function\n",
    "# Output: candidate_list, non_candidate_list\n",
    "\n",
    "def LC_candidates(df, step_size):\n",
    "    df['LC'] = df['predictions'].apply(lambda x: least_confidence_calculation(x))\n",
    "    df = df.sort_values(by='LC')\n",
    "    candidate_list = list(df.index[:step_size])\n",
    "    non_candidate_list = list(df.index[step_size+1:])\n",
    "    return candidate_list, non_candidate_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate least confidence score from predictions of one sentence\n",
    "# Input: predictions in shape [#words, #labels] as np.array\n",
    "# Output: min margin value \n",
    "\n",
    "@jit(nopython=True)\n",
    "\n",
    "def least_confidence_calculation(predictions):\n",
    "    least_confidence_list = []\n",
    "    for word in predictions: # word level\n",
    "        pred_1st = np.partition(word,-1)[-1] # highest prediction\n",
    "        least_confidence_list.append(pred_1st)\n",
    "    return min(least_confidence_list) # minimum prediction of words in sentence is weakest link "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Margin Sampling (MA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Margin strategy to obtain candidates\n",
    "# Input: df (incl. predictions) + step_size -> Utilizes max_entropy_calculation function\n",
    "# Output: candidate_list, non_candidate_list\n",
    "\n",
    "def MA_candidates(df, step_size):\n",
    "    df['margin'] = df['predictions'].apply(lambda x: margin_calculation(x))\n",
    "    df = df.sort_values(by='margin')\n",
    "    candidate_list = list(df.index[:step_size])\n",
    "    non_candidate_list = list(df.index[step_size+1:])\n",
    "    return candidate_list, non_candidate_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate min margin score from predictions of one sentence\n",
    "# Input: predictions in shape [#words, #labels] as np.array\n",
    "# Output: min margin value \n",
    "\n",
    "@jit(nopython=True)\n",
    "\n",
    "def margin_calculation(predictions):\n",
    "    margin_list = []\n",
    "    for word in predictions: # word level\n",
    "        pred_1st = np.partition(word,-1)[-1] # highest prediction\n",
    "        pred_2nd = np.partition(word,-2)[-2] # second-highest prediction\n",
    "        margin = pred_1st - pred_2nd\n",
    "        margin_list.append(margin)\n",
    "    return min(margin_list) # minimum margin of word in sentence is weakest link "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Entropy Sampling (EN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entropy strategy to obtain candidates\n",
    "# Input: df (incl. predictions) + step_size -> Utilizes max_entropy_calculation function\n",
    "# Output: candidate_list, non_candidate_list\n",
    "\n",
    "def EN_candidates(df, step_size):\n",
    "    df['entropy'] = df['predictions'].apply(lambda x: max_entropy_calculation(x))\n",
    "    df = df.sort_values(by='entropy',ascending=False)\n",
    "    candidate_list = list(df.index[:step_size])\n",
    "    non_candidate_list = list(df.index[step_size+1:])\n",
    "    return candidate_list, non_candidate_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate max entropy score from predictions of one sentence\n",
    "# Input: predictions in shape [#words, #labels] as np.array\n",
    "# Output: max entropy value \n",
    "\n",
    "@jit(nopython=True)\n",
    "def max_entropy_calculation(predictions):\n",
    "    entropy_list = []\n",
    "\n",
    "    for word in predictions: # word basis\n",
    "        entropy = 0\n",
    "        for label_prob in word: #label basis    \n",
    "            added_entropy = -label_prob*np.log(label_prob)\n",
    "            entropy += added_entropy\n",
    "        entropy_list.append(entropy)\n",
    "\n",
    "    return max(entropy_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Dataset shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to shift dataset after inference and batched mapping\n",
    "# input dataset has to be input_dataset['train']\n",
    "# i.e.: input_train_dataset = seed dataset (60 sentences)\n",
    "# i.e.: input_inference_dataset = remaining dataset (540 sentences)\n",
    "\n",
    "\n",
    "def dataset_shift(input_train_dataset, input_inference_dataset, inference_config, step_size, AL_STRATEGY):\n",
    "    inference_model = AutoModelForTokenClassification.from_pretrained(inference_config)\n",
    "    df_inference = general_inference(input_inference_dataset, inference_model)\n",
    "    if (AL_STRATEGY == 'LC'):\n",
    "        candidate_list, non_candidate_list = LC_candidates(df_inference, step_size)\n",
    "    elif (AL_STRATEGY == 'MA'):\n",
    "        candidate_list, non_candidate_list = MA_candidates(df_inference, step_size)\n",
    "    elif (AL_STRATEGY == 'EN'):\n",
    "        candidate_list, non_candidate_list = EN_candidates(df_inference, step_size)\n",
    "    \n",
    "    added_inference_dataset = input_inference_dataset.select(candidate_list) # select candidates\n",
    "    \n",
    "    output_train_dataset = concatenate_datasets([input_train_dataset, added_inference_dataset]) # add candidates to train_dataset\n",
    "    output_inference_dataset = input_inference_dataset.select(non_candidate_list) # remaining rows become new inference_dataset \n",
    "\n",
    "    return output_train_dataset, output_inference_dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finetuning metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataCollator -> adds padding\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    "metric = evaluate.load(\"seqeval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write a function to strip special tokens and convert to strings -> true_labels + true_predictions\n",
    "# compare true_labels with true_predictions using seqeval\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "    # Remove ignored index (special tokens) and convert to labels\n",
    "    true_labels = [[label_names[l] for l in label if l != -100] # single labels\n",
    "        for label in labels] # scale towards every label in sentence\n",
    "    true_predictions = [\n",
    "        [label_names[p] for (p, l) in zip(prediction, label) if l != -100] # single labels, zip with l in order to delete -100 tokens\n",
    "        for prediction, label in zip(predictions, labels) # scale towards every label in sentence\n",
    "    ]\n",
    "    all_metrics = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": all_metrics[\"overall_precision\"],\n",
    "        \"recall\": all_metrics[\"overall_recall\"],\n",
    "        \"f1\": all_metrics[\"overall_f1\"],\n",
    "        \"accuracy\": all_metrics[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionarys for better mapping\n",
    "\n",
    "id2label = {str(i): label for i, label in enumerate(label_names)}\n",
    "label2id = {v: k for k, v in id2label.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'id2label' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [28], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# define model\u001b[39;00m\n\u001b[0;32m      3\u001b[0m model \u001b[39m=\u001b[39m AutoModelForTokenClassification\u001b[39m.\u001b[39mfrom_pretrained(\n\u001b[0;32m      4\u001b[0m     MODEL_CONFIG,\n\u001b[1;32m----> 5\u001b[0m     id2label\u001b[39m=\u001b[39mid2label,\n\u001b[0;32m      6\u001b[0m     label2id\u001b[39m=\u001b[39mlabel2id,\n\u001b[0;32m      7\u001b[0m )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'id2label' is not defined"
     ]
    }
   ],
   "source": [
    "# define model\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    MODEL_CONFIG,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and save models over all al_steps locally\n",
    "\n",
    "model_path_list = []\n",
    "\n",
    "for al_step in trange(iterations): # loop over all AL iteration steps [0 = 10%, 1=20%, etc.]\n",
    "    \n",
    "    # select query strategies\n",
    "    if(AL_STRATEGY == 'RS'):\n",
    "        al_train_dataset = sliced_datasets[al_step]\n",
    "\n",
    "    elif(AL_STRATEGY == 'LC' or AL_STRATEGY == 'MA' or AL_STRATEGY == 'EN'):\n",
    "        if(al_step == 0): # seed init in first iteration\n",
    "            al_train_dataset = tokenized_datasets['train'].select(range(0,step_size)) # seed train dataset\n",
    "            al_inference_dataset = tokenized_datasets['train'].select(range(step_size+1,tokenized_datasets['train'].num_rows)) # seed inference dataset\n",
    "        else:\n",
    "            inference_config = \"./bert-finetuned-ner-\" + str((al_step)*STEP_PERCENT) + \"percent\" #not al_step +1, as inference on last trained model\n",
    "            al_train_dataset, al_inference_dataset = dataset_shift(al_train_dataset, al_inference_dataset, inference_config, step_size, AL_STRATEGY)\n",
    "    \n",
    "    else:\n",
    "        print(\"AL strategy not existent\")\n",
    "        break\n",
    "\n",
    "    args = TrainingArguments(\n",
    "        \"cache/bert-finetuned-ner-\" + str((al_step+1)*STEP_PERCENT) + \"percent\", # al_step = 0 -> 10% data\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        learning_rate=2e-5,\n",
    "        num_train_epochs=5, #changed to 5\n",
    "        weight_decay=0.01,\n",
    "        per_device_train_batch_size = 12, # added\n",
    "        per_device_eval_batch_size = 12, # added\n",
    "        load_best_model_at_end = True,\n",
    "        seed = 2022,\n",
    "        push_to_hub=False,\n",
    "        # report_to=\"wandb\",\n",
    "        # run_name=\"AL-\" + AL_STRATEGY \"-\" + str((al_step+1)*STEP_PERCENT) + \"percent-seed\" + str(SEED)\n",
    "        # hub_model_id=\"dxiao/bert-finetuned-ner-\" + str((al_step+1)*10) + \"percent\"\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        train_dataset=al_train_dataset,\n",
    "        eval_dataset=tokenized_datasets[\"validation\"],\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "        tokenizer=tokenizer\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    model_path = \"./bert-finetuned-ner-\" + str((al_step+1)*STEP_PERCENT) + \"percent\"\n",
    "    model_path_list.append(model_path)\n",
    "    trainer.save_model(model_path)\n",
    "    %rm -rf ./cache*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model directly from local path\n",
    "\n",
    "dataset_test = load_dataset(\"dxiao/requirements-ner\", split=\"test\")\n",
    "task_evaluator = evaluator(\"token-classification\")\n",
    "\n",
    "results = []\n",
    "for model in model_path_list:\n",
    "    results.append(\n",
    "        task_evaluator.compute(\n",
    "            model_or_pipeline=model, data=dataset_test, metric=\"seqeval\"\n",
    "            )\n",
    "        )\n",
    "\n",
    "df = pd.DataFrame(results, index=model_path_list)\n",
    "df[[\"overall_f1\", \"overall_accuracy\", \"total_time_in_seconds\", \"samples_per_second\", \"latency_in_seconds\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preliminary plotting of F1 learning curve\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "score_f1 = [0] + list(round(df[\"overall_f1\"],3)) # add 0 at the start of non-tuned model\n",
    "steps_relative = list(np.linspace(0,100,iterations+1))\n",
    "\n",
    "plt.plot(steps_relative, score_f1, label=AL_STRATEGY)\n",
    "plt.axhline(y=max(score_f1), color='orange', label='Maximum')\n",
    "plt.xlim([0, 100])\n",
    "plt.ylim([0, 1])\n",
    "plt.title(f'Preliminary results: {AL_STRATEGY} strategy with {STEP_PERCENT}% steps')\n",
    "plt.xlabel('Percentage of manually labeled data')\n",
    "plt.xticks(list(np.linspace(0, 100, 11)))\n",
    "plt.ylabel('F1 score')\n",
    "plt.yticks(list(np.linspace(0, 1, 11)))\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(color='grey', linewidth=0.2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_f1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0069e35276b4b16933fc948b1a9f9ae6be664e95860c7c07ee003b238b1460ad"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
