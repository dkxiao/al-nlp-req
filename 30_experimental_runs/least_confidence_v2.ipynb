{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Least confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ToDos:\n",
    "\n",
    "# 1. Implement model inference based on finetuned transformer (on cloud) [DONE]\n",
    "# 2. Aggregate inference of entities in one sentence into aggregated certainty score (ACS) [DONE]\n",
    "# 3. Detokenize dataset from word tokens into sentence to be labeled (will this worsen results?) [DONE]\n",
    "# 4. Scale ACS inference to hf dataset and rank by ACS score [DONE]\n",
    "# 5. Scale to selected dataset slices and return as new input dataset [DONE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "from datasets import load_dataset\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "import torch\n",
    "from numba import jit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration dxiao--requirements-ner-a9d27206730c3bd0\n",
      "Found cached dataset json (C:/Users/dekai/.cache/huggingface/datasets/dxiao___json/dxiao--requirements-ner-a9d27206730c3bd0/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73a9abdb067a42079083b86300941a7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "input_model = \"dxiao/bert-finetuned-ner-100percent\"\n",
    "input_dataset = 'dxiao/requirements-ner-id'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(input_model)\n",
    "xiao_data = load_dataset(input_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generalizable inference\n",
    "# Input: dataset + inference_model -> Utilizes inference function\n",
    "# Output: df (incl. predictions)\n",
    "\n",
    "def general_inference(dataset, inference_model):\n",
    "    df = dataset.to_pandas()\n",
    "    df['predictions'] = df['tokens'].apply(lambda x: inference(x.tolist(),inference_model))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform inference on sentences\n",
    "# Input: word-tokenized sentence as list + model\n",
    "# Output: predictions [#words, #labels] as np.array\n",
    "\n",
    "def inference(sentence, inference_model):\n",
    "    encoding = tokenizer(sentence, return_tensors=\"pt\", truncation=True, is_split_into_words=True) #same params as in evalrun\n",
    "    outputs = inference_model(**encoding)\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.nn.functional.softmax(logits, dim=-1)[0]\n",
    "    predictions = predictions.detach().numpy()\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Least confidence strategy obtain candidates\n",
    "# Input: df (incl. predictions) + step_size -> Utilizes least_confidence_calculation function\n",
    "# Output: candidate_list, non_candidate_list\n",
    "\n",
    "def LC_candidates(df, step_size):\n",
    "    df['LC'] = df['predictions'].apply(lambda x: least_confidence_calculation(x))\n",
    "    df = df.sort_values(by='LC')\n",
    "    candidate_list = list(df.index[:step_size])\n",
    "    non_candidate_list = list(df.index[step_size+1:])\n",
    "    return candidate_list, non_candidate_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate least confidence score from predictions of one sentence\n",
    "# Input: predictions in shape [#words, #labels] as np.array\n",
    "# Output: min margin value \n",
    "\n",
    "@jit(nopython=True)\n",
    "\n",
    "def least_confidence_calculation(predictions):\n",
    "    least_confidence_list = []\n",
    "    for word in predictions: # word level\n",
    "        pred_1st = np.partition(word,-1)[-1] # highest prediction\n",
    "        least_confidence_list.append(pred_1st)\n",
    "    return min(least_confidence_list) # minimum prediction of words in sentence is weakest link "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to perform inference_aggregation and batched mapping\n",
    "# input dataset has to be input_dataset['train']\n",
    "# i.e.: input_train_dataset = seed dataset (60 sentences)\n",
    "# i.e.: input_inference_dataset = remaining dataset (540 sentences)\n",
    "\n",
    "from datasets import concatenate_datasets\n",
    "\n",
    "def inference_aggregation(input_train_dataset, input_inference_dataset, step_size):\n",
    "    candidate_list, non_candidate_list = top_candidates(input_inference_dataset, step_size)\n",
    "    added_inference_dataset = input_inference_dataset.select(candidate_list) # select candidates\n",
    "    \n",
    "    output_train_dataset = concatenate_datasets([input_train_dataset, added_inference_dataset]) # add candidates to train_dataset\n",
    "    output_inference_dataset = input_inference_dataset.select(non_candidate_list) # remaining rows become new inference_dataset \n",
    "\n",
    "    return output_train_dataset, output_inference_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = inference_aggregation(\n",
    "    xiao_data['train'].select(range(60)), xiao_data['train'].select(range(61,xiao_data['train'].num_rows)), step_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13 (tags/v3.9.13:6de2ca5, May 17 2022, 16:36:42) [MSC v.1929 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0069e35276b4b16933fc948b1a9f9ae6be664e95860c7c07ee003b238b1460ad"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
