{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example sentence transformed throughout thesis methdology section\n",
    "\n",
    "# 1. Original CanSat sentence\n",
    "# 2. Word wise split with IOB format\n",
    "# 3. Tokenized WordPiece split with word-ids\n",
    "# 4. Word-id aligned\n",
    "# 5. Early model iteration inference with logit and prediction\n",
    "# 6. Late model iteration inference with logit and prediction\n",
    "# 7. Query strategy calculations on sentence\n",
    "# 8. F1 evaluation metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# HuggingFace packages\n",
    "from datasets import load_dataset\n",
    "from datasets import concatenate_datasets\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import Trainer\n",
    "from transformers import TrainingArguments\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "from transformers import AutoModelForTokenClassification\n",
    "import evaluate\n",
    "from evaluate import evaluator\n",
    "from tqdm.notebook import trange\n",
    "\n",
    "import torch\n",
    "from numba import jit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [[\"The\", \"Payload\", \"shall\", \"resist\", \"an\", \"acceleration\", \"of\", \"at\", \"least\", \"60Gs\", \"of\", \"shock\"],[\"test\",\"test\"]]\n",
    "ner_tags = [[0,9,0,1,0,3,0,5,6,8,0,0],[0,0]]\n",
    "data = {'tokens': tokens, 'ner_tags': ner_tags}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, 0, 1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 9, 9, 10, 11, None]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3. Tokenized WordPiece split with word-ids\n",
    "test = tokenizer(data['tokens'], truncation=False, is_split_into_words=True)\n",
    "test.word_ids(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alignment function\n",
    "# special tokens = -100\n",
    "# replace tokens with have been split from B into B + I\n",
    "\n",
    "def align_labels_with_tokens(labels, word_ids):\n",
    "    new_labels = []\n",
    "    current_word_id = None\n",
    "    for word_id in word_ids:\n",
    "        if word_id != current_word_id: # check if it is not same word, just split into two\n",
    "            current_word_id = word_id \n",
    "            label = -100 if word_id is None else labels[word_id]\n",
    "            new_labels.append(label)\n",
    "        elif word_id is None:\n",
    "            # Special token\n",
    "            new_labels.append(-100)\n",
    "        else:\n",
    "            # Same word as previous token\n",
    "            label = labels[word_id]\n",
    "            # If the label is B-XXX we change it to I-XXX\n",
    "            if label % 2 == 1:\n",
    "                label += 1\n",
    "            new_labels.append(label)\n",
    "\n",
    "    return new_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaling align_labels_with_tokens function towards entire datasets by using lists in lists [[]]\n",
    "\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True) # similar to word_ids\n",
    "\n",
    "    all_labels = examples[\"ner_tags\"] # similar to labels but recursive [[ner_tag1],[ner_tag2],[ner_tag3]]\n",
    "    new_labels = []\n",
    "    for i, labels in enumerate(all_labels): # i = iterative number, labels = [ner_tag1]\n",
    "        word_ids = tokenized_inputs.word_ids(i) # takes word_ids from tokenizer magic based on iterative number \n",
    "        new_labels.append(align_labels_with_tokens(labels, word_ids)) # creates [[new_labels1],[new_labels2],[new_labels3]]\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = new_labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-100, 0, 9, 10, 0, 1, 0, 3, 0, 5, 6, 8, 8, 8, 0, 0, -100]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4. Word-id aligned\n",
    "tokenize_and_align_labels(data)['labels'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_model = AutoModelForTokenClassification.from_pretrained(\"dxiao/bert-finetuned-ner-100percent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(sentence, inference_model):\n",
    "    encoding = tokenizer(sentence, return_tensors=\"pt\", truncation=True, is_split_into_words=True) #same params as in evalrun\n",
    "    outputs = inference_model(**encoding)\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.nn.functional.softmax(logits, dim=-1)[0]\n",
    "    predictions = predictions.detach().numpy()\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9.8735946e-01, 1.4781930e-03, 3.4862154e-04, 2.1089052e-03,\n",
       "       1.2852497e-03, 1.0302414e-03, 5.0840829e-04, 7.2974205e-04,\n",
       "       3.0446865e-03, 1.0607493e-03, 1.0458402e-03], dtype=float32)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use inference from display inference. Examples can be tinkered with!\n",
    "inference(data['tokens'][0], inference_model)[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0069e35276b4b16933fc948b1a9f9ae6be664e95860c7c07ee003b238b1460ad"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
