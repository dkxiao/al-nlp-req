{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entropy query strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ToDos:\n",
    "\n",
    "# 1. Display dataframe\n",
    "# 2. Implement entropy calculation -> logit and prediction (logit through softmax layer) on token level\n",
    "# 2.1. Get logit score of tokens\n",
    "# 2.2. Turn logit into predictions with softmax\n",
    "# 2.3. perform entropy math on predictions\n",
    "\n",
    "# In General: perform query strategies based on token level and not on word level -> consider all tokens. even if they are tagged 0\n",
    "# rewrite LC into token level strategy later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "from datasets import load_dataset\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "import torch\n",
    "from numba import jit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TYPE IN EXAMPLE HERE\n",
    "example = \"The Payload shall resist an acceleration of at least 60Gs of shocks\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_model = \"dxiao/bert-finetuned-ner-100percent\"\n",
    "input_dataset = 'dxiao/requirements-ner-id'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(input_model)\n",
    "xiao_data = load_dataset(input_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = ['The', 'Payload', 'shall', 'resist', 'an', 'acceleration', 'of', 'at' ,'least', '60Gs' ,'of', 'shocks']\n",
    "example = ['The', 'probability', 'of', 'undetected', 'frame', 'error', 'for', 'the', 'COM', 'TCC', 'Terminal', 'uplink', 'shall', 'be', '<','19', '-', 'Oct.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rough prototype pipeline\n",
    "\n",
    "encoding = tokenizer(example, return_tensors=\"pt\", truncation=True, is_split_into_words=True) #same params as in evalrun\n",
    "outputs = model(**encoding)\n",
    "logits = outputs.logits\n",
    "predictions = torch.nn.functional.softmax(logits, dim=-1)[0]\n",
    "\n",
    "# -> use of datasets directly with list of words. No need for detokenization anymore!\n",
    "# -> Output of entire entropy strategy should be the id_list of the top candidates \n",
    "\n",
    "# TODO: entropy math on predictions -> entropy score for each sentence\n",
    "# TODO: make fast using numpy\n",
    "# TODO: scale up with datasets -> leverage batched mapping. to entropy calc of dataset. Turn dataset into df and extract candidate indices\n",
    "# TODO: instead of combining words into sentence and then tokenizing one by one, try batched_mapping from evalrun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generalizable inference\n",
    "# Input: dataset + inference_model -> Utilizes inference function\n",
    "# Output: df (incl. predictions)\n",
    "\n",
    "def general_inference(dataset, inference_model):\n",
    "    df = dataset.to_pandas()\n",
    "    df['predictions'] = df['tokens'].apply(lambda x: inference(x.tolist(),inference_model))\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform inference on sentences\n",
    "# Input: word-tokenized sentence as list + model\n",
    "# Output: predictions [#words, #labels] as np.array\n",
    "\n",
    "def inference(sentence, inference_model):\n",
    "    encoding = tokenizer(sentence, return_tensors=\"pt\", truncation=True, is_split_into_words=True) #same params as in evalrun\n",
    "    outputs = inference_model(**encoding)\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.nn.functional.softmax(logits, dim=-1)[0]\n",
    "    predictions = predictions.detach().numpy()\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entropy strategy obtain candidates\n",
    "# Input: df (incl. predictions) + step_size -> Utilizes max_entropy_calculation function\n",
    "# Output: candidate_list, non_candidate_list\n",
    "\n",
    "def EN_candidates(df, step_size):\n",
    "    df['entropy'] = df['predictions'].apply(lambda x: max_entropy_calculation(x))\n",
    "    df = df.sort_values(by='entropy',ascending=False)\n",
    "    candidate_list = list(df.index[:step_size])\n",
    "    non_candidate_list = list(df.index[step_size+1:])\n",
    "    return candidate_list, non_candidate_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate max entropy score from predictions of one sentence\n",
    "# Input: predictions in shape [#words, #labels] as np.array\n",
    "# Output: max entropy value \n",
    "\n",
    "@jit(nopython=True)\n",
    "def max_entropy_calculation(predictions):\n",
    "    entropy_list = []\n",
    "\n",
    "    for word in predictions: # word basis\n",
    "        entropy = 0\n",
    "        for label_prob in word: #label basis    \n",
    "            added_entropy = -label_prob*np.log(label_prob)\n",
    "            entropy += added_entropy\n",
    "        entropy_list.append(entropy)\n",
    "\n",
    "    return max(entropy_list)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13 (tags/v3.9.13:6de2ca5, May 17 2022, 16:36:42) [MSC v.1929 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0069e35276b4b16933fc948b1a9f9ae6be664e95860c7c07ee003b238b1460ad"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
