{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregating model inference and scaling to unlabeled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ToDos:\n",
    "\n",
    "# 1. Implement model inference based on finetuned transformer (on cloud) [DONE]\n",
    "# 2. Aggregate inference of entities in one sentence into aggregated certainty score (ACS) [DONE]\n",
    "# 3. Detokenize dataset from word tokens into sentence to be labeled (will this worsen results?) [DONE]\n",
    "# 4. Scale ACS inference to hf dataset and rank by ACS score [DONE]\n",
    "# 5. Scale to selected dataset slices and return as new input dataset [DONE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "from datasets import load_dataset\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration dxiao--requirements-ner-a9d27206730c3bd0\n",
      "Found cached dataset json (C:/Users/dekai/.cache/huggingface/datasets/dxiao___json/dxiao--requirements-ner-a9d27206730c3bd0/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "167ee98d7ab24bafa903f5c6da64b786",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# model config\n",
    "\n",
    "input_model = \"dxiao/bert-finetuned-ner-10percent\"\n",
    "input_dataset = 'dxiao/requirements-ner-id'\n",
    "step_size = 60\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(input_model)\n",
    "pipe = pipeline(\"ner\", model=model, tokenizer=tokenizer, aggregation_strategy=\"simple\")\n",
    "xiao_data = load_dataset(input_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detokenize dataset\n",
    "import re\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer as Detok\n",
    "\n",
    "def detokenize(token_list):\n",
    "    detokenizer = Detok()\n",
    "    text = detokenizer.detokenize(token_list)\n",
    "    text = re.sub('\\s*,\\s*', ', ', text)\n",
    "    text = re.sub('\\s*\\.\\s*', '.', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate average of all certainty scores in ner_results\n",
    "# might be calculated faster -> Vectorize with np\n",
    "def calculate_ACS(text):\n",
    "    ner_results = pipe(text)\n",
    "    certainty_score_list = []\n",
    "    for i in ner_results:\n",
    "        certainty_score_list.append(i['score'])\n",
    "    if not certainty_score_list: # if list is empty\n",
    "        aggregated_certainty_score = 0\n",
    "    else:\n",
    "        aggregated_certainty_score = sum(certainty_score_list)/len(certainty_score_list)\n",
    "    return aggregated_certainty_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset input has to be 'train' set\n",
    "\n",
    "def top_candidates(dataset, step_size):\n",
    "    candidate_list = []\n",
    "    df = dataset.to_pandas()\n",
    "    # detokenize tokens into new text column\n",
    "    df['text'] = df['tokens'].apply(lambda x: detokenize(x))\n",
    "    # inference calculation of ACS and new ACS columns\n",
    "    df['ACS'] = df['text'].apply(lambda x: calculate_ACS(x)) # takes ~30s for 600 rows\n",
    "    # rank from lowest ACS to highest\n",
    "    df = df.sort_values(by='ACS')\n",
    "    candidate_list = list(df.index[:step_size])\n",
    "    non_candidate_list = list(df.index[step_size+1:])\n",
    "    return candidate_list, non_candidate_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to perform inference_aggregation and batched mapping\n",
    "# input dataset has to be input_dataset['train']\n",
    "# i.e.: input_train_dataset = seed dataset (60 sentences)\n",
    "# i.e.: input_inference_dataset = remaining dataset (540 sentences)\n",
    "\n",
    "from datasets import concatenate_datasets\n",
    "\n",
    "def inference_aggregation(input_train_dataset, input_inference_dataset, step_size):\n",
    "    candidate_list, non_candidate_list = top_candidates(input_inference_dataset, step_size)\n",
    "    added_inference_dataset = input_inference_dataset.select(candidate_list) # select candidates\n",
    "    \n",
    "    output_train_dataset = concatenate_datasets([input_train_dataset, added_inference_dataset]) # add candidates to train_dataset\n",
    "    output_inference_dataset = input_inference_dataset.select(non_candidate_list) # remaining rows become new inference_dataset \n",
    "\n",
    "    return output_train_dataset, output_inference_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = inference_aggregation(\n",
    "    xiao_data['train'].select(range(60)), xiao_data['train'].select(range(61,xiao_data['train'].num_rows)), step_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([317, 406, 407, 408, 409, 410, 411, 405, 412, 414],\n",
       " [416,\n",
       "  417,\n",
       "  418,\n",
       "  420,\n",
       "  413,\n",
       "  421,\n",
       "  404,\n",
       "  400,\n",
       "  382,\n",
       "  383,\n",
       "  384,\n",
       "  385,\n",
       "  388,\n",
       "  389,\n",
       "  401,\n",
       "  390,\n",
       "  392,\n",
       "  393,\n",
       "  394,\n",
       "  397,\n",
       "  398,\n",
       "  399,\n",
       "  391,\n",
       "  422,\n",
       "  423,\n",
       "  425,\n",
       "  447,\n",
       "  448,\n",
       "  449,\n",
       "  450,\n",
       "  452,\n",
       "  453,\n",
       "  446,\n",
       "  454,\n",
       "  457,\n",
       "  458,\n",
       "  459,\n",
       "  460,\n",
       "  461,\n",
       "  462,\n",
       "  455,\n",
       "  445,\n",
       "  444,\n",
       "  443,\n",
       "  427,\n",
       "  428,\n",
       "  429,\n",
       "  430,\n",
       "  431,\n",
       "  432,\n",
       "  433,\n",
       "  434,\n",
       "  435,\n",
       "  436,\n",
       "  437,\n",
       "  438,\n",
       "  440,\n",
       "  441,\n",
       "  442,\n",
       "  381,\n",
       "  380,\n",
       "  379,\n",
       "  378,\n",
       "  319,\n",
       "  320,\n",
       "  321,\n",
       "  322,\n",
       "  324,\n",
       "  325,\n",
       "  634,\n",
       "  326,\n",
       "  328,\n",
       "  329,\n",
       "  330,\n",
       "  332,\n",
       "  333,\n",
       "  334,\n",
       "  327,\n",
       "  316,\n",
       "  315,\n",
       "  314,\n",
       "  297,\n",
       "  298,\n",
       "  299,\n",
       "  300,\n",
       "  301,\n",
       "  302,\n",
       "  303,\n",
       "  305,\n",
       "  306,\n",
       "  307,\n",
       "  308,\n",
       "  309,\n",
       "  310,\n",
       "  312,\n",
       "  313,\n",
       "  335,\n",
       "  463,\n",
       "  336,\n",
       "  339,\n",
       "  363,\n",
       "  364,\n",
       "  366,\n",
       "  367,\n",
       "  368,\n",
       "  369,\n",
       "  362,\n",
       "  370,\n",
       "  372,\n",
       "  373,\n",
       "  374,\n",
       "  375,\n",
       "  376,\n",
       "  377,\n",
       "  371,\n",
       "  361,\n",
       "  360,\n",
       "  359,\n",
       "  340,\n",
       "  341,\n",
       "  342,\n",
       "  343,\n",
       "  344,\n",
       "  345,\n",
       "  346,\n",
       "  347,\n",
       "  348,\n",
       "  350,\n",
       "  351,\n",
       "  353,\n",
       "  354,\n",
       "  355,\n",
       "  356,\n",
       "  337,\n",
       "  295,\n",
       "  464,\n",
       "  467,\n",
       "  577,\n",
       "  578,\n",
       "  580,\n",
       "  581,\n",
       "  582,\n",
       "  583,\n",
       "  576,\n",
       "  584,\n",
       "  586,\n",
       "  587,\n",
       "  588,\n",
       "  589,\n",
       "  590,\n",
       "  591,\n",
       "  585,\n",
       "  592,\n",
       "  575,\n",
       "  573,\n",
       "  558,\n",
       "  559,\n",
       "  560,\n",
       "  561,\n",
       "  562,\n",
       "  563,\n",
       "  574,\n",
       "  564,\n",
       "  566,\n",
       "  567,\n",
       "  568,\n",
       "  569,\n",
       "  571,\n",
       "  572,\n",
       "  565,\n",
       "  594,\n",
       "  595,\n",
       "  596,\n",
       "  619,\n",
       "  620,\n",
       "  621,\n",
       "  622,\n",
       "  624,\n",
       "  625,\n",
       "  618,\n",
       "  626,\n",
       "  628,\n",
       "  629,\n",
       "  630,\n",
       "  631,\n",
       "  632,\n",
       "  633,\n",
       "  627,\n",
       "  617,\n",
       "  616,\n",
       "  613,\n",
       "  597,\n",
       "  598,\n",
       "  600,\n",
       "  601,\n",
       "  602,\n",
       "  603,\n",
       "  604,\n",
       "  605,\n",
       "  606,\n",
       "  607,\n",
       "  608,\n",
       "  609,\n",
       "  610,\n",
       "  611,\n",
       "  612,\n",
       "  557,\n",
       "  556,\n",
       "  555,\n",
       "  554,\n",
       "  495,\n",
       "  496,\n",
       "  497,\n",
       "  500,\n",
       "  501,\n",
       "  503,\n",
       "  492,\n",
       "  504,\n",
       "  506,\n",
       "  508,\n",
       "  509,\n",
       "  510,\n",
       "  511,\n",
       "  512,\n",
       "  505,\n",
       "  491,\n",
       "  490,\n",
       "  489,\n",
       "  470,\n",
       "  471,\n",
       "  473,\n",
       "  475,\n",
       "  476,\n",
       "  477,\n",
       "  480,\n",
       "  481,\n",
       "  482,\n",
       "  483,\n",
       "  484,\n",
       "  485,\n",
       "  486,\n",
       "  487,\n",
       "  488,\n",
       "  513,\n",
       "  466,\n",
       "  514,\n",
       "  518,\n",
       "  540,\n",
       "  541,\n",
       "  542,\n",
       "  543,\n",
       "  544,\n",
       "  545,\n",
       "  539,\n",
       "  546,\n",
       "  548,\n",
       "  549,\n",
       "  550,\n",
       "  551,\n",
       "  552,\n",
       "  553,\n",
       "  547,\n",
       "  538,\n",
       "  537,\n",
       "  536,\n",
       "  519,\n",
       "  520,\n",
       "  521,\n",
       "  523,\n",
       "  525,\n",
       "  526,\n",
       "  527,\n",
       "  528,\n",
       "  529,\n",
       "  530,\n",
       "  531,\n",
       "  532,\n",
       "  533,\n",
       "  534,\n",
       "  535,\n",
       "  517,\n",
       "  294,\n",
       "  635,\n",
       "  292,\n",
       "  113,\n",
       "  114,\n",
       "  115,\n",
       "  116,\n",
       "  117,\n",
       "  118,\n",
       "  112,\n",
       "  119,\n",
       "  121,\n",
       "  122,\n",
       "  123,\n",
       "  124,\n",
       "  126,\n",
       "  127,\n",
       "  120,\n",
       "  128,\n",
       "  111,\n",
       "  109,\n",
       "  94,\n",
       "  95,\n",
       "  96,\n",
       "  97,\n",
       "  98,\n",
       "  99,\n",
       "  110,\n",
       "  100,\n",
       "  102,\n",
       "  104,\n",
       "  105,\n",
       "  106,\n",
       "  107,\n",
       "  108,\n",
       "  101,\n",
       "  93,\n",
       "  129,\n",
       "  131,\n",
       "  152,\n",
       "  153,\n",
       "  154,\n",
       "  156,\n",
       "  157,\n",
       "  158,\n",
       "  151,\n",
       "  159,\n",
       "  162,\n",
       "  163,\n",
       "  165,\n",
       "  166,\n",
       "  167,\n",
       "  168,\n",
       "  161,\n",
       "  130,\n",
       "  150,\n",
       "  148,\n",
       "  132,\n",
       "  133,\n",
       "  134,\n",
       "  135,\n",
       "  136,\n",
       "  137,\n",
       "  149,\n",
       "  139,\n",
       "  141,\n",
       "  142,\n",
       "  143,\n",
       "  144,\n",
       "  293,\n",
       "  147,\n",
       "  140,\n",
       "  169,\n",
       "  92,\n",
       "  90,\n",
       "  34,\n",
       "  35,\n",
       "  36,\n",
       "  37,\n",
       "  38,\n",
       "  39,\n",
       "  32,\n",
       "  40,\n",
       "  42,\n",
       "  44,\n",
       "  45,\n",
       "  46,\n",
       "  47,\n",
       "  48,\n",
       "  41,\n",
       "  50,\n",
       "  30,\n",
       "  26,\n",
       "  1,\n",
       "  2,\n",
       "  4,\n",
       "  6,\n",
       "  9,\n",
       "  10,\n",
       "  27,\n",
       "  11,\n",
       "  14,\n",
       "  15,\n",
       "  16,\n",
       "  17,\n",
       "  24,\n",
       "  25,\n",
       "  12,\n",
       "  91,\n",
       "  51,\n",
       "  53,\n",
       "  73,\n",
       "  74,\n",
       "  76,\n",
       "  77,\n",
       "  78,\n",
       "  79,\n",
       "  72,\n",
       "  80,\n",
       "  82,\n",
       "  83,\n",
       "  84,\n",
       "  87,\n",
       "  88,\n",
       "  89,\n",
       "  81,\n",
       "  52,\n",
       "  71,\n",
       "  69,\n",
       "  54,\n",
       "  55,\n",
       "  56,\n",
       "  57,\n",
       "  59,\n",
       "  60,\n",
       "  70,\n",
       "  61,\n",
       "  63,\n",
       "  64,\n",
       "  65,\n",
       "  66,\n",
       "  67,\n",
       "  68,\n",
       "  62,\n",
       "  170,\n",
       "  146,\n",
       "  233,\n",
       "  224,\n",
       "  223,\n",
       "  260,\n",
       "  261,\n",
       "  222,\n",
       "  221,\n",
       "  220,\n",
       "  262,\n",
       "  218,\n",
       "  263,\n",
       "  264,\n",
       "  217,\n",
       "  216,\n",
       "  215,\n",
       "  214,\n",
       "  213,\n",
       "  265,\n",
       "  212,\n",
       "  211,\n",
       "  210,\n",
       "  266,\n",
       "  268,\n",
       "  208,\n",
       "  207,\n",
       "  251,\n",
       "  225,\n",
       "  226,\n",
       "  227,\n",
       "  228,\n",
       "  248,\n",
       "  247,\n",
       "  246,\n",
       "  245,\n",
       "  252,\n",
       "  244,\n",
       "  254,\n",
       "  243,\n",
       "  255,\n",
       "  241,\n",
       "  240,\n",
       "  239,\n",
       "  269,\n",
       "  256,\n",
       "  238,\n",
       "  258,\n",
       "  237,\n",
       "  236,\n",
       "  235,\n",
       "  234,\n",
       "  171,\n",
       "  259,\n",
       "  232,\n",
       "  231,\n",
       "  230,\n",
       "  229,\n",
       "  257,\n",
       "  206,\n",
       "  250,\n",
       "  271,\n",
       "  270,\n",
       "  283,\n",
       "  189,\n",
       "  187,\n",
       "  186,\n",
       "  185,\n",
       "  184,\n",
       "  183,\n",
       "  284,\n",
       "  182,\n",
       "  285,\n",
       "  282,\n",
       "  286,\n",
       "  288,\n",
       "  180,\n",
       "  289,\n",
       "  179,\n",
       "  178,\n",
       "  290,\n",
       "  176,\n",
       "  175,\n",
       "  174,\n",
       "  173,\n",
       "  291,\n",
       "  287,\n",
       "  191,\n",
       "  190,\n",
       "  197,\n",
       "  198,\n",
       "  277,\n",
       "  196,\n",
       "  278,\n",
       "  280,\n",
       "  275,\n",
       "  201,\n",
       "  195,\n",
       "  205,\n",
       "  194,\n",
       "  202,\n",
       "  274,\n",
       "  193,\n",
       "  273,\n",
       "  281,\n",
       "  199,\n",
       "  200,\n",
       "  499,\n",
       "  524,\n",
       "  29,\n",
       "  304,\n",
       "  138,\n",
       "  203,\n",
       "  387,\n",
       "  296,\n",
       "  465,\n",
       "  204,\n",
       "  579,\n",
       "  181,\n",
       "  85,\n",
       "  338,\n",
       "  615,\n",
       "  468,\n",
       "  419,\n",
       "  474,\n",
       "  0,\n",
       "  23,\n",
       "  249,\n",
       "  614,\n",
       "  177,\n",
       "  311,\n",
       "  352,\n",
       "  396,\n",
       "  593,\n",
       "  426,\n",
       "  125,\n",
       "  599,\n",
       "  18,\n",
       "  155,\n",
       "  86,\n",
       "  402,\n",
       "  75,\n",
       "  8,\n",
       "  164,\n",
       "  493,\n",
       "  439,\n",
       "  502,\n",
       "  172,\n",
       "  49,\n",
       "  479,\n",
       "  279,\n",
       "  58,\n",
       "  498,\n",
       "  160,\n",
       "  522,\n",
       "  349,\n",
       "  22,\n",
       "  456,\n",
       "  188,\n",
       "  209,\n",
       "  451,\n",
       "  403,\n",
       "  5,\n",
       "  43,\n",
       "  507,\n",
       "  3,\n",
       "  494,\n",
       "  272,\n",
       "  478,\n",
       "  424,\n",
       "  323,\n",
       "  365,\n",
       "  395,\n",
       "  357,\n",
       "  21,\n",
       "  472,\n",
       "  7,\n",
       "  386,\n",
       "  623,\n",
       "  276,\n",
       "  28,\n",
       "  13,\n",
       "  516,\n",
       "  267,\n",
       "  145,\n",
       "  469,\n",
       "  331,\n",
       "  33,\n",
       "  31,\n",
       "  192,\n",
       "  318,\n",
       "  219,\n",
       "  19,\n",
       "  570,\n",
       "  20,\n",
       "  242,\n",
       "  358,\n",
       "  253,\n",
       "  103,\n",
       "  515])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_candidates(xiao_data['train'],10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_xiao_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [10], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# return id of top 60 (variable) lowest ACS scores is list\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m lowest_ACS \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(df_xiao_train[\u001b[39m'\u001b[39m\u001b[39mid\u001b[39m\u001b[39m'\u001b[39m][:\u001b[39m60\u001b[39m])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_xiao_train' is not defined"
     ]
    }
   ],
   "source": [
    "# return id of top 60 (variable) lowest ACS scores is list\n",
    "lowest_ACS = list(df_xiao_train['id'][:60])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TYPE IN EXAMPLE HERE\n",
    "example = \"The Payload shall resist an acceleration of at least 60Gs of shocks\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "# displays inference for one sentence\n",
    "def display_inference(text):\n",
    "    ner_results = pipe(text)\n",
    "    spacy_pipe = spacy.blank(\"en\")\n",
    "    doc = spacy_pipe(text)\n",
    "\n",
    "    ents = []\n",
    "    for i in ner_results:\n",
    "        span = doc.char_span(i['start'], i['end'], label=i['entity_group']) #None if mapping issue\n",
    "        ents.append(span)\n",
    "    doc.ents = ents \n",
    "\n",
    "    colors = {\"ENT\": \"#C5BDF4\", \"ACT\": \"#FFD882\", \"ATTR\": \"#D9FBAD\", \"RELOP\": \"#FFDAF9\", \"QUANT\": \"#C2F2F6\"}\n",
    "    options = {\"ents\": ['ENT', 'ACT', 'ATTR', 'RELOP', 'QUANT'], \"colors\": colors}\n",
    "\n",
    "    displacy.render(doc, style = 'ent', options = options)\n",
    "\n",
    "    for i in ner_results:\n",
    "        print(f'{i}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">The \n",
       "<mark class=\"entity\" style=\"background: #C5BDF4; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Payload\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ENT</span>\n",
       "</mark>\n",
       " shall \n",
       "<mark class=\"entity\" style=\"background: #FFD882; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    resist\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ACT</span>\n",
       "</mark>\n",
       " an \n",
       "<mark class=\"entity\" style=\"background: #D9FBAD; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    acceleration\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ATTR</span>\n",
       "</mark>\n",
       " of \n",
       "<mark class=\"entity\" style=\"background: #FFDAF9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    at least\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">RELOP</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #C2F2F6; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    60Gs\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">QUANT</span>\n",
       "</mark>\n",
       " of shocks</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'entity_group': 'ENT', 'score': 0.9986803, 'word': 'Payload', 'start': 4, 'end': 11}\n",
      "{'entity_group': 'ACT', 'score': 0.99624497, 'word': 'resist', 'start': 18, 'end': 24}\n",
      "{'entity_group': 'ATTR', 'score': 0.99129945, 'word': 'acceleration', 'start': 28, 'end': 40}\n",
      "{'entity_group': 'RELOP', 'score': 0.9969168, 'word': 'at least', 'start': 44, 'end': 52}\n",
      "{'entity_group': 'QUANT', 'score': 0.998516, 'word': '60Gs', 'start': 53, 'end': 57}\n"
     ]
    }
   ],
   "source": [
    "input_model = \"dxiao/bert-finetuned-ner-100percent\"\n",
    "input_dataset = 'dxiao/requirements-ner-id'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(input_model)\n",
    "pipe = pipeline(\"ner\", model=model, tokenizer=tokenizer, aggregation_strategy=\"simple\")\n",
    "\n",
    "display_inference(example)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0069e35276b4b16933fc948b1a9f9ae6be664e95860c7c07ee003b238b1460ad"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
