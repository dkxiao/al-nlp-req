{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full evaluation for distilroberta-base on go-emotions multilabel classification dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dekai\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from evaluate import evaluator\n",
    "from transformers import AutoModelForSequenceClassification, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No config specified, defaulting to: go_emotions/simplified\n",
      "Found cached dataset go_emotions (C:/Users/dekai/.cache/huggingface/datasets/go_emotions/simplified/0.0.0/2637cfdd4e64d30249c3ed2150fa2b9d279766bfcd6a809b9f085c61a90d776d)\n",
      "Loading cached shuffled indices for dataset at C:/Users/dekai/.cache/huggingface/datasets/go_emotions/simplified/0.0.0/2637cfdd4e64d30249c3ed2150fa2b9d279766bfcd6a809b9f085c61a90d776d\\cache-d760ed6ac0fb0dce.arrow\n"
     ]
    }
   ],
   "source": [
    "data = load_dataset(\"go_emotions\", split=\"test\").shuffle(seed=42).select(range(1000))\n",
    "task_evaluator = evaluator(\"text-classification\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'labels', 'id'],\n",
       "    num_rows: 1000\n",
       "})"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': Value(dtype='string', id=None), 'labels': Sequence(feature=ClassLabel(num_classes=28, names=['admiration', 'amusement', 'anger', 'annoyance', 'approval', 'caring', 'confusion', 'curiosity', 'desire', 'disappointment', 'disapproval', 'disgust', 'embarrassment', 'excitement', 'fear', 'gratitude', 'grief', 'joy', 'love', 'nervousness', 'optimism', 'pride', 'realization', 'relief', 'remorse', 'sadness', 'surprise', 'neutral'], id=None), length=-1, id=None), 'id': Value(dtype='string', id=None)}\n"
     ]
    }
   ],
   "source": [
    "print(data.features)\n",
    "\n",
    "labels = {'admiration': 0,\n",
    "'amusement': 1, \n",
    "'anger': 2, \n",
    "'annoyance': 3, \n",
    "'approval': 4, \n",
    "'caring': 5, \n",
    "'confusion': 6, \n",
    "'curiosity': 7, \n",
    "'desire': 8, \n",
    "'disappointment': 9, \n",
    "'disapproval': 10, \n",
    "'disgust': 11, \n",
    "'embarrassment': 12, \n",
    "'excitement': 13, \n",
    "'fear': 14, \n",
    "'gratitude': 15, \n",
    "'grief': 16, \n",
    "'joy': 17, \n",
    "'love': 18, \n",
    "'nervousness': 19, \n",
    "'optimism': 20, \n",
    "'pride': 21, \n",
    "'realization': 22, \n",
    "'relief': 23, \n",
    "'remorse': 24, \n",
    "'sadness': 25, \n",
    "'surprise': 26, \n",
    "'neutral': 27}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.rename_column(\"labels\", \"label_column\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'roberta.pooler.dense.weight', 'lm_head.decoder.weight', 'roberta.pooler.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at distilroberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: 'LABEL_0'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [48], line 20\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# # 1. Pass a model name or path\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39m# eval_results = task_evaluator.compute(\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39m#     model_or_pipeline='distilroberta-base',\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     16\u001b[0m \n\u001b[0;32m     17\u001b[0m \u001b[39m# 3. Pass an instantiated pipeline \u001b[39;00m\n\u001b[0;32m     18\u001b[0m pipe \u001b[39m=\u001b[39m pipeline(\u001b[39m\"\u001b[39m\u001b[39mtext-classification\u001b[39m\u001b[39m\"\u001b[39m, model\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mdistilroberta-base\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m---> 20\u001b[0m eval_results \u001b[39m=\u001b[39m task_evaluator\u001b[39m.\u001b[39;49mcompute(\n\u001b[0;32m     21\u001b[0m     model_or_pipeline\u001b[39m=\u001b[39;49mpipe,\n\u001b[0;32m     22\u001b[0m     data\u001b[39m=\u001b[39;49mdata,\n\u001b[0;32m     23\u001b[0m     label_column\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mlabels\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m     24\u001b[0m     metric\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39maccuracy\u001b[39;49m\u001b[39m\"\u001b[39;49m\n\u001b[0;32m     25\u001b[0m     \u001b[39m# label_mapping=labels\u001b[39;49;00m\n\u001b[0;32m     26\u001b[0m )\n\u001b[0;32m     27\u001b[0m \u001b[39mprint\u001b[39m(eval_results)\n",
      "File \u001b[1;32mc:\\Users\\dekai\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\evaluate\\evaluator\\text_classification.py:105\u001b[0m, in \u001b[0;36mTextClassificationEvaluator.compute\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcompute\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[Dict[\u001b[39mstr\u001b[39m, \u001b[39mfloat\u001b[39m], Any]:\n\u001b[0;32m     42\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     43\u001b[0m \u001b[39m    Compute the metric for a given pipeline and dataset combination.\u001b[39;00m\n\u001b[0;32m     44\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[39m    >>> )\u001b[39;00m\n\u001b[0;32m    103\u001b[0m \u001b[39m    ```\"\"\"\u001b[39;00m\n\u001b[1;32m--> 105\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mcompute(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    107\u001b[0m     \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\dekai\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\evaluate\\evaluator\\base.py:205\u001b[0m, in \u001b[0;36mEvaluator.compute\u001b[1;34m(self, model_or_pipeline, data, metric, tokenizer, feature_extractor, strategy, confidence_level, n_resamples, device, random_state, input_column, label_column, label_mapping)\u001b[0m\n\u001b[0;32m    202\u001b[0m metric_inputs\u001b[39m.\u001b[39mupdate(predictions)\n\u001b[0;32m    204\u001b[0m \u001b[39m# Compute metrics from references and predictions\u001b[39;00m\n\u001b[1;32m--> 205\u001b[0m metric_results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_metric(\n\u001b[0;32m    206\u001b[0m     metric\u001b[39m=\u001b[39;49mmetric,\n\u001b[0;32m    207\u001b[0m     metric_inputs\u001b[39m=\u001b[39;49mmetric_inputs,\n\u001b[0;32m    208\u001b[0m     strategy\u001b[39m=\u001b[39;49mstrategy,\n\u001b[0;32m    209\u001b[0m     confidence_level\u001b[39m=\u001b[39;49mconfidence_level,\n\u001b[0;32m    210\u001b[0m     n_resamples\u001b[39m=\u001b[39;49mn_resamples,\n\u001b[0;32m    211\u001b[0m     random_state\u001b[39m=\u001b[39;49mrandom_state,\n\u001b[0;32m    212\u001b[0m )\n\u001b[0;32m    214\u001b[0m result\u001b[39m.\u001b[39mupdate(metric_results)\n\u001b[0;32m    215\u001b[0m result\u001b[39m.\u001b[39mupdate(perf_results)\n",
      "File \u001b[1;32mc:\\Users\\dekai\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\evaluate\\evaluator\\base.py:343\u001b[0m, in \u001b[0;36mEvaluator.compute_metric\u001b[1;34m(self, metric, metric_inputs, strategy, confidence_level, n_resamples, random_state)\u001b[0m\n\u001b[0;32m    333\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcompute_metric\u001b[39m(\n\u001b[0;32m    334\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    335\u001b[0m     metric: EvaluationModule,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    340\u001b[0m     random_state: Optional[\u001b[39mint\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    341\u001b[0m ):\n\u001b[0;32m    342\u001b[0m     \u001b[39m\"\"\"Compute and return metrics.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 343\u001b[0m     result \u001b[39m=\u001b[39m metric\u001b[39m.\u001b[39mcompute(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmetric_inputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mMETRIC_KWARGS)\n\u001b[0;32m    345\u001b[0m     \u001b[39mif\u001b[39;00m strategy \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbootstrap\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    346\u001b[0m         metric_keys \u001b[39m=\u001b[39m result\u001b[39m.\u001b[39mkeys()\n",
      "File \u001b[1;32mc:\\Users\\dekai\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\evaluate\\module.py:432\u001b[0m, in \u001b[0;36mEvaluationModule.compute\u001b[1;34m(self, predictions, references, **kwargs)\u001b[0m\n\u001b[0;32m    429\u001b[0m compute_kwargs \u001b[39m=\u001b[39m {k: kwargs[k] \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m kwargs \u001b[39mif\u001b[39;00m k \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_feature_names()}\n\u001b[0;32m    431\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39many\u001b[39m(v \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mfor\u001b[39;00m v \u001b[39min\u001b[39;00m inputs\u001b[39m.\u001b[39mvalues()):\n\u001b[1;32m--> 432\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madd_batch(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39minputs)\n\u001b[0;32m    433\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_finalize()\n\u001b[0;32m    435\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcache_file_name \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\dekai\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\evaluate\\module.py:486\u001b[0m, in \u001b[0;36mEvaluationModule.add_batch\u001b[1;34m(self, predictions, references, **kwargs)\u001b[0m\n\u001b[0;32m    484\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(column) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    485\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_enforce_nested_string_type(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcurrent_features[key], column[\u001b[39m0\u001b[39m])\n\u001b[1;32m--> 486\u001b[0m     batch \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcurrent_features\u001b[39m.\u001b[39;49mencode_batch(batch)\n\u001b[0;32m    487\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwriter\u001b[39m.\u001b[39mwrite_batch(batch)\n\u001b[0;32m    488\u001b[0m \u001b[39mexcept\u001b[39;00m (pa\u001b[39m.\u001b[39mArrowInvalid, \u001b[39mTypeError\u001b[39;00m):\n",
      "File \u001b[1;32mc:\\Users\\dekai\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\datasets\\features\\features.py:1631\u001b[0m, in \u001b[0;36mFeatures.encode_batch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m   1629\u001b[0m \u001b[39mfor\u001b[39;00m key, column \u001b[39min\u001b[39;00m batch\u001b[39m.\u001b[39mitems():\n\u001b[0;32m   1630\u001b[0m     column \u001b[39m=\u001b[39m cast_to_python_objects(column)\n\u001b[1;32m-> 1631\u001b[0m     encoded_batch[key] \u001b[39m=\u001b[39m [encode_nested_example(\u001b[39mself\u001b[39m[key], obj) \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m column]\n\u001b[0;32m   1632\u001b[0m \u001b[39mreturn\u001b[39;00m encoded_batch\n",
      "File \u001b[1;32mc:\\Users\\dekai\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\datasets\\features\\features.py:1631\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   1629\u001b[0m \u001b[39mfor\u001b[39;00m key, column \u001b[39min\u001b[39;00m batch\u001b[39m.\u001b[39mitems():\n\u001b[0;32m   1630\u001b[0m     column \u001b[39m=\u001b[39m cast_to_python_objects(column)\n\u001b[1;32m-> 1631\u001b[0m     encoded_batch[key] \u001b[39m=\u001b[39m [encode_nested_example(\u001b[39mself\u001b[39;49m[key], obj) \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m column]\n\u001b[0;32m   1632\u001b[0m \u001b[39mreturn\u001b[39;00m encoded_batch\n",
      "File \u001b[1;32mc:\\Users\\dekai\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\datasets\\features\\features.py:1220\u001b[0m, in \u001b[0;36mencode_nested_example\u001b[1;34m(schema, obj, level)\u001b[0m\n\u001b[0;32m   1217\u001b[0m \u001b[39m# Object with special encoding:\u001b[39;00m\n\u001b[0;32m   1218\u001b[0m \u001b[39m# ClassLabel will convert from string to int, TranslationVariableLanguages does some checks\u001b[39;00m\n\u001b[0;32m   1219\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(schema, (Audio, Image, ClassLabel, TranslationVariableLanguages, Value, _ArrayXD)):\n\u001b[1;32m-> 1220\u001b[0m     \u001b[39mreturn\u001b[39;00m schema\u001b[39m.\u001b[39;49mencode_example(obj) \u001b[39mif\u001b[39;00m obj \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   1221\u001b[0m \u001b[39m# Other object should be directly convertible to a native Arrow type (like Translation and Translation)\u001b[39;00m\n\u001b[0;32m   1222\u001b[0m \u001b[39mreturn\u001b[39;00m obj\n",
      "File \u001b[1;32mc:\\Users\\dekai\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\datasets\\features\\features.py:479\u001b[0m, in \u001b[0;36mValue.encode_example\u001b[1;34m(self, value)\u001b[0m\n\u001b[0;32m    477\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mbool\u001b[39m(value)\n\u001b[0;32m    478\u001b[0m \u001b[39melif\u001b[39;00m pa\u001b[39m.\u001b[39mtypes\u001b[39m.\u001b[39mis_integer(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpa_type):\n\u001b[1;32m--> 479\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mint\u001b[39;49m(value)\n\u001b[0;32m    480\u001b[0m \u001b[39melif\u001b[39;00m pa\u001b[39m.\u001b[39mtypes\u001b[39m.\u001b[39mis_floating(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpa_type):\n\u001b[0;32m    481\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mfloat\u001b[39m(value)\n",
      "\u001b[1;31mValueError\u001b[0m: invalid literal for int() with base 10: 'LABEL_0'"
     ]
    }
   ],
   "source": [
    "\n",
    "# # 1. Pass a model name or path\n",
    "# eval_results = task_evaluator.compute(\n",
    "#     model_or_pipeline='distilroberta-base',\n",
    "#     data=data,\n",
    "#     label_mapping=labels\n",
    "# )\n",
    "\n",
    "# # 2. Pass an instantiated model\n",
    "# model = AutoModelForSequenceClassification.from_pretrained('distilroberta-base')\n",
    "\n",
    "# eval_results = task_evaluator.compute(\n",
    "#     model_or_pipeline=model,\n",
    "#     data=data,\n",
    "#     label_mapping=labels\n",
    "# )\n",
    "\n",
    "# 3. Pass an instantiated pipeline \n",
    "pipe = pipeline(\"text-classification\", model='distilroberta-base')\n",
    "\n",
    "eval_results = task_evaluator.compute(\n",
    "    model_or_pipeline=pipe,\n",
    "    data=data,\n",
    "    label_column=\"labels\",\n",
    "    metric=\"accuracy\"\n",
    "    # label_mapping=labels\n",
    ")\n",
    "print(eval_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0069e35276b4b16933fc948b1a9f9ae6be664e95860c7c07ee003b238b1460ad"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
