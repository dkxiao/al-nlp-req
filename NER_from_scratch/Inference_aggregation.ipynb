{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregating model inference and scaling to unlabeled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ToDos:\n",
    "\n",
    "# 1. Implement model inference based on finetuned transformer (on cloud) [DONE]\n",
    "# 2. Aggregate inference of entities in one sentence into aggregated certainty score (ACS) [DONE]\n",
    "# 3. Detokenize dataset from word tokens into sentence to be labeled (will this worsen results?) [DONE]\n",
    "# 4. Scale ACS inference to hf dataset and rank by ACS score [DONE]\n",
    "# 5. Scale to selected dataset slices and return as new input dataset [DONE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "from datasets import load_dataset\n",
    "from datasets import Dataset\n",
    "from spacy import displacy\n",
    "import pandas as pd\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration dxiao--requirements-ner-a9d27206730c3bd0\n",
      "Found cached dataset json (C:/Users/dekai/.cache/huggingface/datasets/dxiao___json/dxiao--requirements-ner-a9d27206730c3bd0/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53d0a9073e3441f4a6b6360b176481bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# model config\n",
    "\n",
    "input_model = \"dxiao/bert-finetuned-ner-100percent\"\n",
    "input_dataset = 'dxiao/requirements-ner-id'\n",
    "step_size = 60\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(input_model)\n",
    "pipe = pipeline(\"ner\", model=model, tokenizer=tokenizer, aggregation_strategy=\"simple\")\n",
    "xiao_data = load_dataset(input_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detokenize dataset\n",
    "import re\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer as Detok\n",
    "\n",
    "def detokenize(token_list):\n",
    "    detokenizer = Detok()\n",
    "    text = detokenizer.detokenize(token_list)\n",
    "    text = re.sub('\\s*,\\s*', ', ', text)\n",
    "    text = re.sub('\\s*\\.\\s*', '.', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate average of all certainty scores in ner_results\n",
    "# might be calculated faster -> Vectorize with np\n",
    "def calculate_ACS(text):\n",
    "    ner_results = pipe(text)\n",
    "    certainty_score_list = []\n",
    "    for i in ner_results:\n",
    "        certainty_score_list.append(i['score'])\n",
    "    aggregated_certainty_score = sum(certainty_score_list)/len(certainty_score_list)\n",
    "    return aggregated_certainty_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'tokens', 'tags', 'ner_tags', 'text', 'ACS'],\n",
       "    num_rows: 60\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# transform dataset into dataframe\n",
    "df_xiao_train = xiao_data['train'].to_pandas()\n",
    "# detokenize tokens into new text column\n",
    "df_xiao_train['text'] = df_xiao_train['tokens'].apply(lambda x: detokenize(x))\n",
    "# inference calculation of ACS and new ACS columns\n",
    "df_xiao_train['ACS'] = df_xiao_train['text'].apply(lambda x: calculate_ACS(x)) # takes ~30s for 600 rows\n",
    "# rank from lowest ACS to highest\n",
    "df_xiao_train = df_xiao_train.sort_values(by='ACS')\n",
    "# transform dataframe back to dataset\n",
    "xiao_data_train_new = Dataset.from_pandas(df_xiao_train[:step_size])\n",
    "# drop extra columns from transformation\n",
    "xiao_data_train_new = xiao_data_train_new.remove_columns('__index_level_0__')\n",
    "xiao_data_train_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return id of top 60 (variable) lowest ACS scores is list\n",
    "lowest_ACS = list(df_xiao_train['id'][:60])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lowest_ACS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter function for selected ids [id_list] to be mapped onto a dataset\n",
    "def filter_ids(row,id_list):\n",
    "    for i in id_list:\n",
    "        if(row == i):\n",
    "            return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:/Users/dekai/.cache/huggingface/datasets/dxiao___json/dxiao--requirements-ner-a9d27206730c3bd0/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab\\cache-269dcb83466dfa42.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'tokens', 'tags', 'ner_tags'],\n",
       "    num_rows: 0\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# select slide with the lowest ACS scores -> filter function always cuts off after 615. no matter size of dataset\n",
    "filtered_data = xiao_data['train'].filter(lambda x: filter_ids(x['id'],[616,617,618]))\n",
    "filtered_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[512,\n",
       " 257,\n",
       " 386,\n",
       " 4,\n",
       " 134,\n",
       " 137,\n",
       " 9,\n",
       " 267,\n",
       " 16,\n",
       " 17,\n",
       " 280,\n",
       " 537,\n",
       " 536,\n",
       " 27,\n",
       " 414,\n",
       " 416,\n",
       " 163,\n",
       " 36,\n",
       " 421,\n",
       " 35,\n",
       " 423,\n",
       " 46,\n",
       " 47,\n",
       " 561,\n",
       " 433,\n",
       " 434,\n",
       " 52,\n",
       " 566,\n",
       " 183,\n",
       " 440,\n",
       " 55,\n",
       " 446,\n",
       " 68,\n",
       " 197,\n",
       " 198,\n",
       " 589,\n",
       " 212,\n",
       " 349,\n",
       " 94,\n",
       " 478,\n",
       " 481,\n",
       " 97,\n",
       " 357,\n",
       " 614,\n",
       " 103,\n",
       " 361,\n",
       " 235,\n",
       " 492,\n",
       " 237,\n",
       " 496,\n",
       " 626,\n",
       " 499,\n",
       " 628,\n",
       " 245,\n",
       " 118,\n",
       " 632,\n",
       " 253,\n",
       " 506,\n",
       " 509,\n",
       " 510]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# difference between lists -> why are there 3 elements difference?\n",
    "list(set(lowest_ACS) - set(filtered_data['id']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TYPE IN EXAMPLE HERE\n",
    "example = \"The Payload shall resist an acceleration of at least 60Gs of shocks\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# displays inference for one sentence\n",
    "def display_inference(text):\n",
    "    ner_results = pipe(text)\n",
    "    spacy_pipe = spacy.blank(\"en\")\n",
    "    doc = spacy_pipe(example)\n",
    "\n",
    "    ents = []\n",
    "    for i in ner_results:\n",
    "        span = doc.char_span(i['start'], i['end'], label=i['entity_group']) #None if mapping issue\n",
    "        ents.append(span)\n",
    "    doc.ents = ents \n",
    "\n",
    "    colors = {\"ENT\": \"#C5BDF4\", \"ACT\": \"#FFD882\", \"ATTR\": \"#D9FBAD\", \"RELOP\": \"#FFDAF9\", \"QUANT\": \"#C2F2F6\"}\n",
    "    options = {\"ents\": ['ENT', 'ACT', 'ATTR', 'RELOP', 'QUANT'], \"colors\": colors}\n",
    "\n",
    "    displacy.render(doc, style = 'ent', options = options)\n",
    "\n",
    "    for i in ner_results:\n",
    "        print(f'{i}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0069e35276b4b16933fc948b1a9f9ae6be664e95860c7c07ee003b238b1460ad"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
