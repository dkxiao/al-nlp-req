{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregating model inference and scaling to unlabeled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ToDos:\n",
    "\n",
    "# 1. Implement model inference based on finetuned transformer (on cloud) [DONE]\n",
    "# 2. Aggregate inference of entities in one sentence into aggregated certainty score (ACS) [DONE]\n",
    "# 3. Detokenize dataset from word tokens into sentence to be labeled (will this worsen results?) [DONE]\n",
    "# 4. Scale ACS inference to hf dataset and rank by ACS score [DONE]\n",
    "# 5. Scale to selected dataset slices and return as new input dataset [DONE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "from datasets import load_dataset\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration dxiao--requirements-ner-a9d27206730c3bd0\n",
      "Found cached dataset json (C:/Users/dekai/.cache/huggingface/datasets/dxiao___json/dxiao--requirements-ner-a9d27206730c3bd0/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d69b3f9aaf944eaade2dfdf15f55b87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# model config\n",
    "\n",
    "input_model = \"dxiao/bert-finetuned-ner-10percent\"\n",
    "input_dataset = 'dxiao/requirements-ner-id'\n",
    "step_size = 60\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(input_model)\n",
    "pipe = pipeline(\"ner\", model=model, tokenizer=tokenizer, aggregation_strategy=\"simple\")\n",
    "xiao_data = load_dataset(input_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detokenize dataset\n",
    "import re\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer as Detok\n",
    "\n",
    "def detokenize(token_list):\n",
    "    detokenizer = Detok()\n",
    "    text = detokenizer.detokenize(token_list)\n",
    "    text = re.sub('\\s*,\\s*', ', ', text)\n",
    "    text = re.sub('\\s*\\.\\s*', '.', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate average of all certainty scores in ner_results\n",
    "# might be calculated faster -> Vectorize with np\n",
    "def calculate_ACS(text):\n",
    "    ner_results = pipe(text)\n",
    "    certainty_score_list = []\n",
    "    for i in ner_results:\n",
    "        certainty_score_list.append(i['score'])\n",
    "    if not certainty_score_list: # if list is empty\n",
    "        aggregated_certainty_score = 0\n",
    "    else:\n",
    "        aggregated_certainty_score = sum(certainty_score_list)/len(certainty_score_list)\n",
    "    return aggregated_certainty_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'tokens', 'tags', 'ner_tags', 'text', 'ACS'],\n",
       "    num_rows: 60\n",
       "})"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# transform dataset into dataframe\n",
    "df_xiao_train = xiao_data['train'].to_pandas()\n",
    "# detokenize tokens into new text column\n",
    "df_xiao_train['text'] = df_xiao_train['tokens'].apply(lambda x: detokenize(x))\n",
    "# inference calculation of ACS and new ACS columns\n",
    "df_xiao_train['ACS'] = df_xiao_train['text'].apply(lambda x: calculate_ACS(x)) # takes ~30s for 600 rows\n",
    "# rank from lowest ACS to highest\n",
    "df_xiao_train = df_xiao_train.sort_values(by='ACS')\n",
    "# transform dataframe back to dataset\n",
    "xiao_data_train_new = Dataset.from_pandas(df_xiao_train[:step_size])\n",
    "# drop extra columns from transformation\n",
    "xiao_data_train_new = xiao_data_train_new.remove_columns('__index_level_0__')\n",
    "xiao_data_train_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "test, non_test = top_candidates(xiao_data['train'], step_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset input has to be 'train' set\n",
    "\n",
    "def top_candidates(dataset, step_size):\n",
    "    candidate_list = []\n",
    "    df = dataset.to_pandas()\n",
    "    # detokenize tokens into new text column\n",
    "    df['text'] = df['tokens'].apply(lambda x: detokenize(x))\n",
    "    # inference calculation of ACS and new ACS columns\n",
    "    df['ACS'] = df['text'].apply(lambda x: calculate_ACS(x)) # takes ~30s for 600 rows\n",
    "    # rank from lowest ACS to highest\n",
    "    df = df.sort_values(by='ACS')\n",
    "    candidate_list = list(df.index[:step_size])\n",
    "    non_candidate_list = list(df.index[step_size+1:])\n",
    "    return candidate_list, non_candidate_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select certain rows in dataset\n",
    "test_data = xiao_data['train'].select(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import concatenate_datasets\n",
    "\n",
    "def inference_aggregation(input_train_dataset, input_inference_dataset, step_size):\n",
    "    candidate_list, non_candidate_list = top_candidates(input_inference_dataset)\n",
    "    added_inference_dataset = input_inference_dataset.select(candidate_list) # select candidates\n",
    "    \n",
    "    output_train_dataset = concatenate_datasets(input_train_dataset, added_inference_dataset) # add candidates to train_dataset\n",
    "    output_inference_dataset = input_inference_dataset.select(non_candidate_list) # remaining rows become new inference_dataset \n",
    "\n",
    "    return output_train_dataset, output_inference_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to perform inference_aggregation and batched mapping\n",
    "# input dataset has to be input_dataset['train']\n",
    "# i.e.: input_train_dataset = seed dataset (60 sentences)\n",
    "# i.e.: input_inference_dataset = remaining dataset (540 sentences)\n",
    "\n",
    "#input dataset is missing [ACS] column\n",
    "\n",
    "from datasets import concatenate_datasets\n",
    "\n",
    "def inference_aggregation2(input_train_dataset, input_inference_dataset, step_size):\n",
    "    # transform dataset into dataframe\n",
    "    df_inference_dataset = input_inference_dataset.to_pandas()\n",
    "\n",
    "    # detokenize tokens into new text column\n",
    "    df_inference_dataset['text'] = df_inference_dataset['tokens'].apply(lambda x: detokenize(x))\n",
    "\n",
    "    # inference calculation of ACS and new ACS columns\n",
    "    df_inference_dataset['ACS'] = df_inference_dataset['text'].apply(lambda x: calculate_ACS(x)) # takes ~30s for 600 rows\n",
    "\n",
    "    # rank from lowest ACS to highest\n",
    "    df_inference_dataset = df_inference_dataset.sort_values(by='ACS')\n",
    "\n",
    "    # transform dataframe back to dataset\n",
    "    added_train_dataset = Dataset.from_pandas(df_inference_dataset[:step_size]) # new added 10% step with lowest ACS\n",
    "    output_inference_dataset = Dataset.from_pandas(df_inference_dataset[step_size+1:]) # remaining 90% of dataset for inference\n",
    "\n",
    "    # drop extra columns from transformation\n",
    "    added_train_dataset = added_train_dataset.remove_columns(['__index_level_0__','ACS','text'])\n",
    "    output_inference_dataset = output_inference_dataset.remove_columns('__index_level_0__')\n",
    "    \n",
    "    # concat datasets\n",
    "    # output_train_dataset = concatenate_datasets([input_train_dataset,added_train_dataset])\n",
    "    # return output_train_dataset, output_inference_dataset\n",
    "\n",
    "    return added_train_dataset, output_inference_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: resolve concatenation issue due to extra labels. Decide which labels to keep for iterative use. \n",
    "# Output label of train should be same as input labels\n",
    "# Input datatset: ['id', 'tokens', 'tags', 'ner_tags']\n",
    "# added 'text' and 'ACS' during processing and ranking\n",
    "# columns are not the issue. Issue is that ner_tags lost its sequence tag and is not int64\n",
    "\n",
    "x, y = inference_aggregation(\n",
    "    xiao_data['train'].select(range(60)), xiao_data['train'].select(range(61,xiao_data['train'].num_rows)), step_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': Value(dtype='int64', id=None),\n",
       " 'tokens': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None),\n",
       " 'tags': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None),\n",
       " 'ner_tags': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None)}"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': Value(dtype='int64', id=None),\n",
       " 'tokens': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None),\n",
       " 'tags': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None),\n",
       " 'ner_tags': Sequence(feature=ClassLabel(names=['O', 'B-ACT', 'I-ACT', 'B-ATTR', 'I-ATTR', 'B-RELOP', 'I-RELOP', 'B-QUANT', 'I-QUANT', 'B-ENT', 'I-ENT'], id=None), length=-1, id=None)}"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xiao_data['train'].select(range(60)).features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The features can't be aligned because the key ner_tags of features {'id': Value(dtype='int64', id=None), 'tokens': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), 'tags': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), 'ner_tags': Sequence(feature=ClassLabel(names=['O', 'B-ACT', 'I-ACT', 'B-ATTR', 'I-ATTR', 'B-RELOP', 'I-RELOP', 'B-QUANT', 'I-QUANT', 'B-ENT', 'I-ENT'], id=None), length=-1, id=None)} has unexpected type - Sequence(feature=ClassLabel(names=['O', 'B-ACT', 'I-ACT', 'B-ATTR', 'I-ATTR', 'B-RELOP', 'I-RELOP', 'B-QUANT', 'I-QUANT', 'B-ENT', 'I-ENT'], id=None), length=-1, id=None) (expected either Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None) or Value(\"null\").",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [91], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m test \u001b[39m=\u001b[39m concatenate_datasets([x,xiao_data[\u001b[39m'\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mselect(\u001b[39mrange\u001b[39;49m(\u001b[39m60\u001b[39;49m))])\n",
      "File \u001b[1;32mc:\\Users\\dekai\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\datasets\\combine.py:173\u001b[0m, in \u001b[0;36mconcatenate_datasets\u001b[1;34m(dsets, info, split, axis)\u001b[0m\n\u001b[0;32m    169\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    170\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnable to concatenate a \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(dsets[\u001b[39m0\u001b[39m])\u001b[39m}\u001b[39;00m\u001b[39m with a \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(dataset)\u001b[39m}\u001b[39;00m\u001b[39m. Expected a list of Dataset objects or a list of IterableDataset objects.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    171\u001b[0m         )\n\u001b[0;32m    172\u001b[0m \u001b[39mif\u001b[39;00m map_style:\n\u001b[1;32m--> 173\u001b[0m     \u001b[39mreturn\u001b[39;00m _concatenate_map_style_datasets(dsets, info\u001b[39m=\u001b[39;49minfo, split\u001b[39m=\u001b[39;49msplit, axis\u001b[39m=\u001b[39;49maxis)\n\u001b[0;32m    174\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    175\u001b[0m     \u001b[39mreturn\u001b[39;00m _concatenate_iterable_datasets(dsets, info\u001b[39m=\u001b[39minfo, split\u001b[39m=\u001b[39msplit, axis\u001b[39m=\u001b[39maxis)\n",
      "File \u001b[1;32mc:\\Users\\dekai\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\datasets\\arrow_dataset.py:5042\u001b[0m, in \u001b[0;36m_concatenate_map_style_datasets\u001b[1;34m(dsets, info, split, axis)\u001b[0m\n\u001b[0;32m   5040\u001b[0m \u001b[39m# Perform checks (and a potentional cast if axis=0)\u001b[39;00m\n\u001b[0;32m   5041\u001b[0m \u001b[39mif\u001b[39;00m axis \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m-> 5042\u001b[0m     _check_if_features_can_be_aligned([dset\u001b[39m.\u001b[39;49mfeatures \u001b[39mfor\u001b[39;49;00m dset \u001b[39min\u001b[39;49;00m dsets])\n\u001b[0;32m   5043\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   5044\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mall\u001b[39m([dset\u001b[39m.\u001b[39mnum_rows \u001b[39m==\u001b[39m dsets[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mnum_rows \u001b[39mfor\u001b[39;00m dset \u001b[39min\u001b[39;00m dsets]):\n",
      "File \u001b[1;32mc:\\Users\\dekai\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\datasets\\features\\features.py:1997\u001b[0m, in \u001b[0;36m_check_if_features_can_be_aligned\u001b[1;34m(features_list)\u001b[0m\n\u001b[0;32m   1995\u001b[0m \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m features\u001b[39m.\u001b[39mitems():\n\u001b[0;32m   1996\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39misinstance\u001b[39m(v, Value) \u001b[39mand\u001b[39;00m v\u001b[39m.\u001b[39mdtype \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mnull\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m name2feature[k] \u001b[39m!=\u001b[39m v:\n\u001b[1;32m-> 1997\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1998\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mThe features can\u001b[39m\u001b[39m\\'\u001b[39;00m\u001b[39mt be aligned because the key \u001b[39m\u001b[39m{\u001b[39;00mk\u001b[39m}\u001b[39;00m\u001b[39m of features \u001b[39m\u001b[39m{\u001b[39;00mfeatures\u001b[39m}\u001b[39;00m\u001b[39m has unexpected type - \u001b[39m\u001b[39m{\u001b[39;00mv\u001b[39m}\u001b[39;00m\u001b[39m (expected either \u001b[39m\u001b[39m{\u001b[39;00mname2feature[k]\u001b[39m}\u001b[39;00m\u001b[39m or Value(\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mnull\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m).\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m   1999\u001b[0m         )\n",
      "\u001b[1;31mValueError\u001b[0m: The features can't be aligned because the key ner_tags of features {'id': Value(dtype='int64', id=None), 'tokens': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), 'tags': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), 'ner_tags': Sequence(feature=ClassLabel(names=['O', 'B-ACT', 'I-ACT', 'B-ATTR', 'I-ATTR', 'B-RELOP', 'I-RELOP', 'B-QUANT', 'I-QUANT', 'B-ENT', 'I-ENT'], id=None), length=-1, id=None)} has unexpected type - Sequence(feature=ClassLabel(names=['O', 'B-ACT', 'I-ACT', 'B-ATTR', 'I-ATTR', 'B-RELOP', 'I-RELOP', 'B-QUANT', 'I-QUANT', 'B-ENT', 'I-ENT'], id=None), length=-1, id=None) (expected either Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None) or Value(\"null\")."
     ]
    }
   ],
   "source": [
    "test = concatenate_datasets([x,xiao_data['train'].select(range(60))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'tokens', 'tags', 'ner_tags'],\n",
       "    num_rows: 575\n",
       "})"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xiao_data['train'].select(range(61,xiao_data['train'].num_rows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = concatenate_datasets([x,y])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return id of top 60 (variable) lowest ACS scores is list\n",
    "lowest_ACS = list(df_xiao_train['id'][:60])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lowest_ACS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter function for selected ids [id_list] to be mapped onto a dataset\n",
    "def filter_ids(row,id_list):\n",
    "    for i in id_list:\n",
    "        if(row == i):\n",
    "            return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:/Users/dekai/.cache/huggingface/datasets/dxiao___json/dxiao--requirements-ner-a9d27206730c3bd0/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab\\cache-269dcb83466dfa42.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'tokens', 'tags', 'ner_tags'],\n",
       "    num_rows: 0\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# select slide with the lowest ACS scores -> filter function always cuts off after 615. no matter size of dataset\n",
    "filtered_data = xiao_data['train'].filter(lambda x: filter_ids(x['id'],[616,617,618]))\n",
    "filtered_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[512,\n",
       " 257,\n",
       " 386,\n",
       " 4,\n",
       " 134,\n",
       " 137,\n",
       " 9,\n",
       " 267,\n",
       " 16,\n",
       " 17,\n",
       " 280,\n",
       " 537,\n",
       " 536,\n",
       " 27,\n",
       " 414,\n",
       " 416,\n",
       " 163,\n",
       " 36,\n",
       " 421,\n",
       " 35,\n",
       " 423,\n",
       " 46,\n",
       " 47,\n",
       " 561,\n",
       " 433,\n",
       " 434,\n",
       " 52,\n",
       " 566,\n",
       " 183,\n",
       " 440,\n",
       " 55,\n",
       " 446,\n",
       " 68,\n",
       " 197,\n",
       " 198,\n",
       " 589,\n",
       " 212,\n",
       " 349,\n",
       " 94,\n",
       " 478,\n",
       " 481,\n",
       " 97,\n",
       " 357,\n",
       " 614,\n",
       " 103,\n",
       " 361,\n",
       " 235,\n",
       " 492,\n",
       " 237,\n",
       " 496,\n",
       " 626,\n",
       " 499,\n",
       " 628,\n",
       " 245,\n",
       " 118,\n",
       " 632,\n",
       " 253,\n",
       " 506,\n",
       " 509,\n",
       " 510]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# difference between lists -> why are there 3 elements difference?\n",
    "list(set(lowest_ACS) - set(filtered_data['id']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TYPE IN EXAMPLE HERE\n",
    "example = \"The Payload shall resist an acceleration of at least 60Gs of shocks\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "# displays inference for one sentence\n",
    "def display_inference(text):\n",
    "    ner_results = pipe(text)\n",
    "    spacy_pipe = spacy.blank(\"en\")\n",
    "    doc = spacy_pipe(text)\n",
    "\n",
    "    ents = []\n",
    "    for i in ner_results:\n",
    "        span = doc.char_span(i['start'], i['end'], label=i['entity_group']) #None if mapping issue\n",
    "        ents.append(span)\n",
    "    doc.ents = ents \n",
    "\n",
    "    colors = {\"ENT\": \"#C5BDF4\", \"ACT\": \"#FFD882\", \"ATTR\": \"#D9FBAD\", \"RELOP\": \"#FFDAF9\", \"QUANT\": \"#C2F2F6\"}\n",
    "    options = {\"ents\": ['ENT', 'ACT', 'ATTR', 'RELOP', 'QUANT'], \"colors\": colors}\n",
    "\n",
    "    displacy.render(doc, style = 'ent', options = options)\n",
    "\n",
    "    for i in ner_results:\n",
    "        print(f'{i}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0069e35276b4b16933fc948b1a9f9ae6be664e95860c7c07ee003b238b1460ad"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
