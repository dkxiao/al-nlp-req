{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entropy Evalrun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cloud install for Colab\n",
    "!pip install datasets\n",
    "!pip install transformers\n",
    "!pip install evaluate\n",
    "!pip install seqeval\n",
    "!pip install numba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Login into huggingface_hub\n",
    "# Token: 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\n",
    "# from huggingface_hub import notebook_login\n",
    "# notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer as Detok\n",
    "\n",
    "# HuggingFace packages\n",
    "from datasets import load_dataset\n",
    "from datasets import concatenate_datasets\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import Trainer\n",
    "from transformers import TrainingArguments\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "from transformers import AutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "import evaluate\n",
    "from evaluate import evaluator\n",
    "from tqdm.notebook import trange\n",
    "\n",
    "import torch\n",
    "from numba import jit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "STEP_PERCENT = 10 # can be changed\n",
    "TOTAL_SIZE = 600\n",
    "DATASET_CONFIG = \"dxiao/requirements-ner-id\"\n",
    "MODEL_CONFIG = \"bert-base-cased\"\n",
    "AL_STRATEGY = 'EN' # LC or RS or EN\n",
    "\n",
    "# parameter calculations\n",
    "iterations = 100/STEP_PERCENT\n",
    "step_size = TOTAL_SIZE/iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration dxiao--requirements-ner-a9d27206730c3bd0\n",
      "Found cached dataset json (C:/Users/dekai/.cache/huggingface/datasets/dxiao___json/dxiao--requirements-ner-a9d27206730c3bd0/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d4e56e49ff646b5a021b5951d1b3667",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# import from huggingface\n",
    "raw_datasets = load_dataset(DATASET_CONFIG)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get label names\n",
    "ner_feature = raw_datasets[\"train\"].features[\"ner_tags\"] \n",
    "label_names = ner_feature.feature.names"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alignment function\n",
    "# special tokens = -100\n",
    "# replace tokens with have been split from B into B + I\n",
    "\n",
    "def align_labels_with_tokens(labels, word_ids):\n",
    "    new_labels = []\n",
    "    current_word_id = None\n",
    "    for word_id in word_ids:\n",
    "        if word_id != current_word_id: # check if it is not same word, just split into two\n",
    "            current_word_id = word_id \n",
    "            label = -100 if word_id is None else labels[word_id]\n",
    "            new_labels.append(label)\n",
    "        elif word_id is None:\n",
    "            # Special token\n",
    "            new_labels.append(-100)\n",
    "        else:\n",
    "            # Same word as previous token\n",
    "            label = labels[word_id]\n",
    "            # If the label is B-XXX we change it to I-XXX\n",
    "            if label % 2 == 1:\n",
    "                label += 1\n",
    "            new_labels.append(label)\n",
    "\n",
    "    return new_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaling align_labels_with_tokens function towards entire datasets by using lists in lists [[]]\n",
    "\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True) # similar to word_ids\n",
    "\n",
    "    all_labels = examples[\"ner_tags\"] # similar to labels but recursive [[ner_tag1],[ner_tag2],[ner_tag3]]\n",
    "    new_labels = []\n",
    "    for i, labels in enumerate(all_labels): # i = iterative number, labels = [ner_tag1]\n",
    "        word_ids = tokenized_inputs.word_ids(i) # takes word_ids from tokenizer magic based on iterative number \n",
    "        new_labels.append(align_labels_with_tokens(labels, word_ids)) # creates [[new_labels1],[new_labels2],[new_labels3]]\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = new_labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batched mapping function -> combines tokenize_and_align_labels() and align_labels_with_tokens()\n",
    "\n",
    "def batched_mapping(input_dataset):\n",
    "    output_dataset = input_dataset.map(\n",
    "    tokenize_and_align_labels,\n",
    "    batched=True,\n",
    "    # remove_columns=raw_datasets['train'].column_names, # keep columns \n",
    "    )\n",
    "    return output_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:/Users/dekai/.cache/huggingface/datasets/dxiao___json/dxiao--requirements-ner-a9d27206730c3bd0/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab\\cache-5992c15d856b607f.arrow\n",
      "Loading cached processed dataset at C:/Users/dekai/.cache/huggingface/datasets/dxiao___json/dxiao--requirements-ner-a9d27206730c3bd0/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab\\cache-e3b73f63a5239f7b.arrow\n",
      "Loading cached processed dataset at C:/Users/dekai/.cache/huggingface/datasets/dxiao___json/dxiao--requirements-ner-a9d27206730c3bd0/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab\\cache-d01a31e78a4352eb.arrow\n"
     ]
    }
   ],
   "source": [
    "# apply batched mapping on train, test and validation\n",
    "\n",
    "tokenized_datasets = batched_mapping(raw_datasets)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slicing dataset into chunks\n",
    "\n",
    "sliced_datasets = {}\n",
    "\n",
    "for i in range(iterations): \n",
    "    sliced_datasets[i] = tokenized_datasets['train'].select(range(0,(i+1)*step_size)) # sliced_datasets[0] = first 10%\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Least Confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detokenize dataset\n",
    "\n",
    "def detokenize(token_list):\n",
    "    detokenizer = Detok()\n",
    "    text = detokenizer.detokenize(token_list)\n",
    "    text = re.sub('\\s*,\\s*', ', ', text)\n",
    "    text = re.sub('\\s*\\.\\s*', '.', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate average of all certainty scores in ner_results\n",
    "# might be calculated faster -> Vectorize with np\n",
    "def calculate_ACS(text):\n",
    "    ner_results = pipe(text)\n",
    "    certainty_score_list = []\n",
    "    for i in ner_results:\n",
    "        certainty_score_list.append(i['score'])\n",
    "    if not certainty_score_list: # if list is empty\n",
    "        aggregated_certainty_score = 0\n",
    "    else:\n",
    "        aggregated_certainty_score = sum(certainty_score_list)/len(certainty_score_list)\n",
    "    return aggregated_certainty_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset input has to be 'train' set\n",
    "\n",
    "def top_candidates(dataset, step_size):\n",
    "    candidate_list = []\n",
    "    df = dataset.to_pandas()\n",
    "    # detokenize tokens into new text column\n",
    "    df['text'] = df['tokens'].apply(lambda x: detokenize(x))\n",
    "    # inference calculation of ACS and new ACS columns\n",
    "    df['ACS'] = df['text'].apply(lambda x: calculate_ACS(x)) # takes ~30s for 600 rows\n",
    "    # rank from lowest ACS to highest\n",
    "    df = df.sort_values(by='ACS')\n",
    "    candidate_list = list(df.index[:step_size])\n",
    "    non_candidate_list = list(df.index[step_size+1:])\n",
    "    return candidate_list, non_candidate_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to perform inference_aggregation and batched mapping\n",
    "# input dataset has to be input_dataset['train']\n",
    "# i.e.: input_train_dataset = seed dataset (60 sentences)\n",
    "# i.e.: input_inference_dataset = remaining dataset (540 sentences)\n",
    "\n",
    "\n",
    "def inference_aggregation(input_train_dataset, input_inference_dataset, step_size):\n",
    "    candidate_list, non_candidate_list = top_candidates(input_inference_dataset, step_size)\n",
    "    added_inference_dataset = input_inference_dataset.select(candidate_list) # select candidates\n",
    "    \n",
    "    output_train_dataset = concatenate_datasets([input_train_dataset, added_inference_dataset]) # add candidates to train_dataset\n",
    "    output_inference_dataset = input_inference_dataset.select(non_candidate_list) # remaining rows become new inference_dataset \n",
    "\n",
    "    return output_train_dataset, output_inference_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generalizable inference\n",
    "# Input: dataset + inference_model -> Utilizes inference function\n",
    "# Output: df (incl. predictions)\n",
    "\n",
    "def general_inference(dataset, inference_model):\n",
    "    df = dataset.to_pandas()\n",
    "    df['predictions'] = df['tokens'].apply(lambda x: inference(x.tolist(),inference_model))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform inference on sentences\n",
    "# Input: word-tokenized sentence as list + model\n",
    "# Output: predictions [#words, #labels] as np.array\n",
    "\n",
    "def inference(sentence, inference_model):\n",
    "    encoding = tokenizer(sentence, return_tensors=\"pt\", truncation=True, is_split_into_words=True) #same params as in evalrun\n",
    "    outputs = inference_model(**encoding)\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.nn.functional.softmax(logits, dim=-1)[0]\n",
    "    predictions = predictions.detach().numpy()\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entropy strategy obtain candidates\n",
    "# Input: df (incl. predictions) + step_size -> Utilizes max_entropy_calculation function\n",
    "# Output: candidate_list, non_candidate_list\n",
    "\n",
    "def EN_candidates(df, step_size):\n",
    "    df['entropy'] = df['predictions'].apply(lambda x: max_entropy_calculation(x))\n",
    "    df = df.sort_values(by='entropy',ascending=False)\n",
    "    candidate_list = list(df.index[:step_size])\n",
    "    non_candidate_list = list(df.index[step_size+1:])\n",
    "    return candidate_list, non_candidate_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate max entropy score from predictions of one sentence\n",
    "# Input: predictions in shape [#words, #labels] as np.array\n",
    "# Output: max entropy value \n",
    "\n",
    "@jit(nopython=True)\n",
    "def max_entropy_calculation(predictions):\n",
    "    entropy_list = []\n",
    "\n",
    "    for word in predictions: # word basis\n",
    "        entropy = 0\n",
    "        for label_prob in word: #label basis    \n",
    "            added_entropy = -label_prob*np.log(label_prob)\n",
    "            entropy += added_entropy\n",
    "        entropy_list.append(entropy)\n",
    "\n",
    "    return max(entropy_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EN_inference_aggregation(input_train_dataset, input_inference_dataset, inference_model, step_size):\n",
    "    df_inference = general_inference(input_inference_dataset, inference_model)\n",
    "    candidate_list, non_candidate_list = EN_candidates(df_inference, step_size)\n",
    "\n",
    "    added_inference_dataset = input_inference_dataset.select(candidate_list) # select candidates\n",
    "    \n",
    "    output_train_dataset = concatenate_datasets([input_train_dataset, added_inference_dataset]) # add candidates to train_dataset\n",
    "    output_inference_dataset = input_inference_dataset.select(non_candidate_list) # remaining rows become new inference_dataset \n",
    "\n",
    "    return output_train_dataset, output_inference_dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finetuning metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataCollator -> adds padding\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    "metric = evaluate.load(\"seqeval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write a function to strip special tokens and convert to strings -> true_labels + true_predictions\n",
    "# compare true_labels with true_predictions using seqeval\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "    # Remove ignored index (special tokens) and convert to labels\n",
    "    true_labels = [[label_names[l] for l in label if l != -100] # single labels\n",
    "        for label in labels] # scale towards every label in sentence\n",
    "    true_predictions = [\n",
    "        [label_names[p] for (p, l) in zip(prediction, label) if l != -100] # single labels, zip with l in order to delete -100 tokens\n",
    "        for prediction, label in zip(predictions, labels) # scale towards every label in sentence\n",
    "    ]\n",
    "    all_metrics = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": all_metrics[\"overall_precision\"],\n",
    "        \"recall\": all_metrics[\"overall_recall\"],\n",
    "        \"f1\": all_metrics[\"overall_f1\"],\n",
    "        \"accuracy\": all_metrics[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionarys for better mapping\n",
    "\n",
    "id2label = {str(i): label for i, label in enumerate(label_names)}\n",
    "label2id = {v: k for k, v in id2label.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    MODEL_CONFIG,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and save models over all al_steps locally\n",
    "\n",
    "model_path_list = []\n",
    "\n",
    "for al_step in trange(iterations): # loop over all AL iteration steps [0 = 10%, 1=20%, etc.]\n",
    "    \n",
    "    # select query strategies\n",
    "    if(AL_STRATEGY == 'RS'):\n",
    "        al_train_dataset = sliced_datasets[al_step]\n",
    "\n",
    "    elif(AL_STRATEGY == 'LC'):\n",
    "        if(al_step == 0): # seed init in first iteration\n",
    "            al_train_dataset = tokenized_datasets['train'].select(range(0,step_size)) # seed train dataset\n",
    "            al_inference_dataset = tokenized_datasets['train'].select(range(step_size+1,tokenized_datasets['train'].num_rows)) # seed inference dataset\n",
    "        else:\n",
    "            inference_config = \"./bert-finetuned-ner-\" + str((al_step)*STEP_PERCENT) + \"percent\" #not al_step +1, as inference on last trained model\n",
    "            inference_model = AutoModelForTokenClassification.from_pretrained(inference_config)\n",
    "            pipe = pipeline(\"ner\", model=inference_model, tokenizer=tokenizer, aggregation_strategy=\"simple\")\n",
    "            al_train_dataset, al_inference_dataset = inference_aggregation(al_train_dataset, al_inference_dataset, step_size)\n",
    "\n",
    "    elif(AL_STRATEGY == 'EN'):\n",
    "        if(al_step == 0): # seed init in first iteration\n",
    "            al_train_dataset = tokenized_datasets['train'].select(range(0,step_size)) # seed train dataset\n",
    "            al_inference_dataset = tokenized_datasets['train'].select(range(step_size+1,tokenized_datasets['train'].num_rows)) # seed inference dataset\n",
    "        else:\n",
    "            inference_config = \"./bert-finetuned-ner-\" + str((al_step)*STEP_PERCENT) + \"percent\" #not al_step +1, as inference on last trained model\n",
    "            inference_model = AutoModelForTokenClassification.from_pretrained(inference_config)\n",
    "            al_train_dataset, al_inference_dataset = EN_inference_aggregation(al_train_dataset, al_inference_dataset, inference_model, step_size)\n",
    "\n",
    "    else:\n",
    "        print(\"AL strategy not existent\")\n",
    "        break\n",
    "\n",
    "    args = TrainingArguments(\n",
    "        \"cache/bert-finetuned-ner-\" + str((al_step+1)*STEP_PERCENT) + \"percent\", # al_step = 0 -> 10% data\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"no\",\n",
    "        learning_rate=2e-5,\n",
    "        num_train_epochs=3,\n",
    "        weight_decay=0.01,\n",
    "        seed = 2022,\n",
    "        push_to_hub=False,\n",
    "        # hub_model_id=\"dxiao/bert-finetuned-ner-\" + str((al_step+1)*10) + \"percent\"\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        train_dataset=al_train_dataset,\n",
    "        eval_dataset=tokenized_datasets[\"validation\"],\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "        tokenizer=tokenizer\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    model_path = \"./bert-finetuned-ner-\" + str((al_step+1)*STEP_PERCENT) + \"percent\"\n",
    "    model_path_list.append(model_path)\n",
    "    trainer.save_model(model_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model directly from local path\n",
    "\n",
    "dataset_test = load_dataset(\"dxiao/requirements-ner\", split=\"test\")\n",
    "task_evaluator = evaluator(\"token-classification\")\n",
    "\n",
    "results = []\n",
    "for model in model_path_list:\n",
    "    results.append(\n",
    "        task_evaluator.compute(\n",
    "            model_or_pipeline=model, data=dataset_test, metric=\"seqeval\"\n",
    "            )\n",
    "        )\n",
    "\n",
    "df = pd.DataFrame(results, index=model_path_list)\n",
    "df[[\"overall_f1\", \"overall_accuracy\", \"total_time_in_seconds\", \"samples_per_second\", \"latency_in_seconds\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preliminary plotting of F1 learning curve\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "score_f1 = [0] + list(round(df[\"overall_f1\"],3)) # add 0 at the start of non-tuned model\n",
    "steps_relative = list(np.linspace(0,100,iterations+1))\n",
    "\n",
    "plt.plot(steps_relative, score_f1, label=AL_STRATEGY)\n",
    "plt.axhline(y=max(score_f1), color='orange', label='Maximum')\n",
    "plt.xlim([0, 100])\n",
    "plt.ylim([0, 1])\n",
    "plt.title(f'Preliminary results: {AL_STRATEGY} strategy with {STEP_PERCENT}% steps')\n",
    "plt.xlabel('Percentage of manually labeled data')\n",
    "plt.xticks(list(np.linspace(0, 100, 11)))\n",
    "plt.ylabel('F1 score')\n",
    "plt.yticks(list(np.linspace(0, 1, 11)))\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(color='grey', linewidth=0.2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_f1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0069e35276b4b16933fc948b1a9f9ae6be664e95860c7c07ee003b238b1460ad"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
