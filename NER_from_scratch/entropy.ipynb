{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entropy query strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ToDos:\n",
    "\n",
    "# 1. Display dataframe\n",
    "# 2. Implement entropy calculation -> logit and prediction (logit through softmax layer) on token level\n",
    "# 2.1. Get logit score of tokens\n",
    "# 2.2. Turn logit into predictions with softmax\n",
    "# 2.3. perform entropy math on predictions\n",
    "\n",
    "# In General: perform query strategies based on token level and not on word level -> consider all tokens. even if they are tagged 0\n",
    "# rewrite LC into token level strategy later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "from datasets import load_dataset\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration dxiao--requirements-ner-a9d27206730c3bd0\n",
      "Found cached dataset json (C:/Users/dekai/.cache/huggingface/datasets/dxiao___json/dxiao--requirements-ner-a9d27206730c3bd0/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79689027c3754c73937e57e255dbb48d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# model config\n",
    "\n",
    "input_model = \"dxiao/bert-finetuned-ner-100percent\"\n",
    "input_dataset = 'dxiao/requirements-ner-id'\n",
    "step_size = 60\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(input_model)\n",
    "# pipe = pipeline(\"ner\", model=model, tokenizer=tokenizer, aggregation_strategy=\"simple\")\n",
    "xiao_data = load_dataset(input_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detokenize dataset\n",
    "import re\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer as Detok\n",
    "\n",
    "def detokenize(token_list):\n",
    "    detokenizer = Detok()\n",
    "    text = detokenizer.detokenize(token_list)\n",
    "    text = re.sub('\\s*,\\s*', ', ', text)\n",
    "    text = re.sub('\\s*\\.\\s*', '.', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate average of all certainty scores in ner_results\n",
    "# might be calculated faster -> Vectorize with np\n",
    "def calculate_entropy(text):\n",
    "    ner_results = pipe(text)\n",
    "    certainty_score_list = []\n",
    "    for i in ner_results:\n",
    "        certainty_score_list.append(i['score'])\n",
    "    if not certainty_score_list: # if list is empty\n",
    "        aggregated_certainty_score = 0\n",
    "    else:\n",
    "        aggregated_certainty_score = sum(certainty_score_list)/len(certainty_score_list)\n",
    "    return aggregated_certainty_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset input has to be 'train' set\n",
    "\n",
    "def top_candidates(dataset, step_size):\n",
    "    candidate_list = []\n",
    "    df = dataset.to_pandas()\n",
    "    # detokenize tokens into new text column\n",
    "    df['text'] = df['tokens'].apply(lambda x: detokenize(x))\n",
    "    # inference calculation of ACS and new ACS columns\n",
    "    df['ACS'] = df['text'].apply(lambda x: calculate_ACS(x)) # takes ~30s for 600 rows\n",
    "    # rank from lowest ACS to highest\n",
    "    df = df.sort_values(by='ACS')\n",
    "    candidate_list = list(df.index[:step_size])\n",
    "    non_candidate_list = list(df.index[step_size+1:])\n",
    "    return candidate_list, non_candidate_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to perform inference_aggregation and batched mapping\n",
    "# input dataset has to be input_dataset['train']\n",
    "# i.e.: input_train_dataset = seed dataset (60 sentences)\n",
    "# i.e.: input_inference_dataset = remaining dataset (540 sentences)\n",
    "\n",
    "from datasets import concatenate_datasets\n",
    "\n",
    "def inference_aggregation(input_train_dataset, input_inference_dataset, step_size):\n",
    "    candidate_list, non_candidate_list = top_candidates(input_inference_dataset, step_size)\n",
    "    added_inference_dataset = input_inference_dataset.select(candidate_list) # select candidates\n",
    "    \n",
    "    output_train_dataset = concatenate_datasets([input_train_dataset, added_inference_dataset]) # add candidates to train_dataset\n",
    "    output_inference_dataset = input_inference_dataset.select(non_candidate_list) # remaining rows become new inference_dataset \n",
    "\n",
    "    return output_train_dataset, output_inference_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = inference_aggregation(\n",
    "    xiao_data['train'].select(range(60)), xiao_data['train'].select(range(61,xiao_data['train'].num_rows)), step_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TYPE IN EXAMPLE HERE\n",
    "example = \"The Payload shall resist an acceleration of at least 60Gs of shocks\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "# displays inference for one sentence\n",
    "def display_inference(text):\n",
    "    ner_results = pipe(text)\n",
    "    spacy_pipe = spacy.blank(\"en\")\n",
    "    doc = spacy_pipe(text)\n",
    "\n",
    "    ents = []\n",
    "    for i in ner_results:\n",
    "        span = doc.char_span(i['start'], i['end'], label=i['entity_group']) #None if mapping issue\n",
    "        ents.append(span)\n",
    "    doc.ents = ents \n",
    "\n",
    "    colors = {\"ENT\": \"#C5BDF4\", \"ACT\": \"#FFD882\", \"ATTR\": \"#D9FBAD\", \"RELOP\": \"#FFDAF9\", \"QUANT\": \"#C2F2F6\"}\n",
    "    options = {\"ents\": ['ENT', 'ACT', 'ATTR', 'RELOP', 'QUANT'], \"colors\": colors}\n",
    "\n",
    "    displacy.render(doc, style = 'ent', options = options)\n",
    "\n",
    "    for i in ner_results:\n",
    "        print(f'{i}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_model = \"dxiao/bert-finetuned-ner-100percent\"\n",
    "input_dataset = 'dxiao/requirements-ner-id'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(input_model)\n",
    "pipe = pipeline(\"ner\", model=model, tokenizer=tokenizer, aggregation_strategy=\"simple\")\n",
    "\n",
    "display_inference(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = ['The', 'Payload', 'shall', 'resist', 'an', 'acceleration', 'of', 'at' ,'least', '60Gs' ,'of', 'shocks']\n",
    "example = ['The', 'probability', 'of', 'undetected', 'frame', 'error', 'for', 'the', 'COM', 'TCC', 'Terminal', 'uplink', 'shall', 'be', '<','19', '-', 'Oct.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rough prototype pipeline\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "encoding = tokenizer(example, return_tensors=\"pt\", truncation=True, is_split_into_words=True) #same params as in evalrun\n",
    "outputs = model(**encoding)\n",
    "logits = outputs.logits\n",
    "predictions = torch.nn.functional.softmax(logits, dim=-1)[0]\n",
    "\n",
    "# -> use of datasets directly with list of words. No need for detokenization anymore!\n",
    "# -> Output of entire entropy strategy should be the id_list of the top candidates \n",
    "\n",
    "# TODO: entropy math on predictions -> entropy score for each sentence\n",
    "# TODO: make fast using numpy\n",
    "# TODO: scale up with datasets -> leverage batched mapping. to entropy calc of dataset. Turn dataset into df and extract candidate indices\n",
    "# TODO: instead of combining words into sentence and then tokenizing one by one, try batched_mapping from evalrun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from numba import jit\n",
    "\n",
    "# perform inference on sentences\n",
    "# Input: word-tokenized sentence as list\n",
    "# Output: predictions [#words, #labels] as np.array\n",
    "\n",
    "def inference(sentence):\n",
    "    encoding = tokenizer(sentence, return_tensors=\"pt\", truncation=True, is_split_into_words=True) #same params as in evalrun\n",
    "    outputs = model(**encoding)\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.nn.functional.softmax(logits, dim=-1)[0]\n",
    "    predictions = predictions.detach().numpy()\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate max entropy score from predictions of one sentence\n",
    "# Input: predictions in shape [#words, #labels] as np.array\n",
    "# Output: max entropy value \n",
    "\n",
    "@jit(nopython=True)\n",
    "def max_entropy_calculation(predictions):\n",
    "    entropy_list = []\n",
    "\n",
    "    for word in predictions: # word basis\n",
    "        entropy = 0\n",
    "        for label_prob in word: #label basis    \n",
    "            added_entropy = -label_prob*np.log(label_prob)\n",
    "            entropy += added_entropy\n",
    "        entropy_list.append(entropy)\n",
    "\n",
    "    return max(entropy_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EN: return stepwise list of top candidates & non-candidates\n",
    "\n",
    "def top_candidates_EN(dataset, step_size):\n",
    "    candidate_list = []\n",
    "    df = dataset.to_pandas()\n",
    "    df['entropy'] = df['tokens'].apply(lambda x: max_entropy_calculation(inference(x.tolist()))) #entropy calc on predictions\n",
    "    df = df.sort_values(by='entropy',ascending=False)\n",
    "    candidate_list = list(df.index[:step_size])\n",
    "    non_candidate_list = list(df.index[step_size+1:])\n",
    "    return candidate_list, non_candidate_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tokens</th>\n",
       "      <th>tags</th>\n",
       "      <th>ner_tags</th>\n",
       "      <th>entropy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>27</td>\n",
       "      <td>[The, probability, of, undetected, frame, erro...</td>\n",
       "      <td>[O, B-ATTR, I-ATTR, I-ATTR, I-ATTR, I-ATTR, O,...</td>\n",
       "      <td>[0, 3, 4, 4, 4, 4, 0, 0, 9, 10, 10, 10, 0, 3, ...</td>\n",
       "      <td>1.909202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>440</th>\n",
       "      <td>440</td>\n",
       "      <td>[The, probability, of, frame, loss, for, the, ...</td>\n",
       "      <td>[O, B-ATTR, I-ATTR, I-ATTR, I-ATTR, O, O, B-EN...</td>\n",
       "      <td>[0, 3, 4, 4, 4, 0, 0, 9, 10, 10, 10, 10, 1, 5,...</td>\n",
       "      <td>1.903301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>614</th>\n",
       "      <td>614</td>\n",
       "      <td>[The, BER, on, physical, channel, for, the, CO...</td>\n",
       "      <td>[O, B-ATTR, I-ATTR, I-ATTR, I-ATTR, O, O, B-EN...</td>\n",
       "      <td>[0, 3, 4, 4, 4, 0, 0, 9, 10, 10, 10, 0, 1, 5, ...</td>\n",
       "      <td>1.866802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>481</th>\n",
       "      <td>481</td>\n",
       "      <td>[Digitisation, for, each, polarisation, shall,...</td>\n",
       "      <td>[B-ATTR, O, O, B-ENT, O, B-ACT, O, O, O, B-REL...</td>\n",
       "      <td>[3, 0, 0, 9, 0, 1, 0, 0, 0, 5, 6, 0, 0, 0, 0, ...</td>\n",
       "      <td>1.831017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>40</td>\n",
       "      <td>[The, algorithm, shall, produce, a, cloud, mas...</td>\n",
       "      <td>[O, O, O, O, O, B-ENT, I-ENT, I-ENT, O, B-ACT,...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 9, 10, 10, 0, 1, 0, 3, 4, 4, 4...</td>\n",
       "      <td>1.797966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[Vertical, profiles, shall, be, measured, in, ...</td>\n",
       "      <td>[B-ENT, I-ENT, O, B-ACT, I-ACT, O, O, B-ATTR, ...</td>\n",
       "      <td>[9, 10, 0, 1, 2, 0, 0, 3, 4, 0, 5, 7, 8, 8, 0,...</td>\n",
       "      <td>0.040787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>182</td>\n",
       "      <td>[The, fine, frequency, cells, for, the, SKA1_L...</td>\n",
       "      <td>[O, B-ATTR, I-ATTR, I-ATTR, O, O, B-ENT, I-ENT...</td>\n",
       "      <td>[0, 3, 4, 4, 0, 0, 9, 10, 0, 1, 0, 7, 0, 0, 0,...</td>\n",
       "      <td>0.040345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360</th>\n",
       "      <td>360</td>\n",
       "      <td>[WFOS, ,, in, spectroscopy, mode, ,, shall, ha...</td>\n",
       "      <td>[B-ENT, O, O, O, O, O, O, B-ACT, O, B-ATTR, I-...</td>\n",
       "      <td>[9, 0, 0, 0, 0, 0, 0, 1, 0, 3, 4, 4, 4, 0, 5, ...</td>\n",
       "      <td>0.035466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>287</th>\n",
       "      <td>287</td>\n",
       "      <td>[The, SKA1_Mid, dishes, ,, when, the, band, 2,...</td>\n",
       "      <td>[O, B-ENT, I-ENT, O, O, O, O, O, O, O, O, O, O...</td>\n",
       "      <td>[0, 9, 10, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,...</td>\n",
       "      <td>0.034628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>45</td>\n",
       "      <td>[The, SKA1_Mid, dishes, ,, when, the, band, 5,...</td>\n",
       "      <td>[O, B-ENT, I-ENT, O, O, O, O, O, O, O, O, O, O...</td>\n",
       "      <td>[0, 9, 10, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,...</td>\n",
       "      <td>0.032924</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>636 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                             tokens  \\\n",
       "27    27  [The, probability, of, undetected, frame, erro...   \n",
       "440  440  [The, probability, of, frame, loss, for, the, ...   \n",
       "614  614  [The, BER, on, physical, channel, for, the, CO...   \n",
       "481  481  [Digitisation, for, each, polarisation, shall,...   \n",
       "40    40  [The, algorithm, shall, produce, a, cloud, mas...   \n",
       "..   ...                                                ...   \n",
       "2      2  [Vertical, profiles, shall, be, measured, in, ...   \n",
       "182  182  [The, fine, frequency, cells, for, the, SKA1_L...   \n",
       "360  360  [WFOS, ,, in, spectroscopy, mode, ,, shall, ha...   \n",
       "287  287  [The, SKA1_Mid, dishes, ,, when, the, band, 2,...   \n",
       "45    45  [The, SKA1_Mid, dishes, ,, when, the, band, 5,...   \n",
       "\n",
       "                                                  tags  \\\n",
       "27   [O, B-ATTR, I-ATTR, I-ATTR, I-ATTR, I-ATTR, O,...   \n",
       "440  [O, B-ATTR, I-ATTR, I-ATTR, I-ATTR, O, O, B-EN...   \n",
       "614  [O, B-ATTR, I-ATTR, I-ATTR, I-ATTR, O, O, B-EN...   \n",
       "481  [B-ATTR, O, O, B-ENT, O, B-ACT, O, O, O, B-REL...   \n",
       "40   [O, O, O, O, O, B-ENT, I-ENT, I-ENT, O, B-ACT,...   \n",
       "..                                                 ...   \n",
       "2    [B-ENT, I-ENT, O, B-ACT, I-ACT, O, O, B-ATTR, ...   \n",
       "182  [O, B-ATTR, I-ATTR, I-ATTR, O, O, B-ENT, I-ENT...   \n",
       "360  [B-ENT, O, O, O, O, O, O, B-ACT, O, B-ATTR, I-...   \n",
       "287  [O, B-ENT, I-ENT, O, O, O, O, O, O, O, O, O, O...   \n",
       "45   [O, B-ENT, I-ENT, O, O, O, O, O, O, O, O, O, O...   \n",
       "\n",
       "                                              ner_tags   entropy  \n",
       "27   [0, 3, 4, 4, 4, 4, 0, 0, 9, 10, 10, 10, 0, 3, ...  1.909202  \n",
       "440  [0, 3, 4, 4, 4, 0, 0, 9, 10, 10, 10, 10, 1, 5,...  1.903301  \n",
       "614  [0, 3, 4, 4, 4, 0, 0, 9, 10, 10, 10, 0, 1, 5, ...  1.866802  \n",
       "481  [3, 0, 0, 9, 0, 1, 0, 0, 0, 5, 6, 0, 0, 0, 0, ...  1.831017  \n",
       "40   [0, 0, 0, 0, 0, 9, 10, 10, 0, 1, 0, 3, 4, 4, 4...  1.797966  \n",
       "..                                                 ...       ...  \n",
       "2    [9, 10, 0, 1, 2, 0, 0, 3, 4, 0, 5, 7, 8, 8, 0,...  0.040787  \n",
       "182  [0, 3, 4, 4, 0, 0, 9, 10, 0, 1, 0, 7, 0, 0, 0,...  0.040345  \n",
       "360  [9, 0, 0, 0, 0, 0, 0, 1, 0, 3, 4, 4, 4, 0, 5, ...  0.035466  \n",
       "287  [0, 9, 10, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,...  0.034628  \n",
       "45   [0, 9, 10, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,...  0.032924  \n",
       "\n",
       "[636 rows x 5 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candidate_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4.3555e-01, 3.6871e-02, 2.8352e-02, 9.7163e-02, 7.8188e-02, 4.0228e-02,\n",
       "         2.5014e-02, 4.1915e-02, 1.2168e-01, 5.6276e-02, 3.8757e-02],\n",
       "        [9.9738e-01, 6.1271e-04, 1.1583e-04, 4.8168e-04, 6.9839e-04, 8.0477e-05,\n",
       "         4.9122e-05, 4.5355e-05, 1.6888e-04, 1.2886e-04, 2.3587e-04],\n",
       "        [1.5931e-04, 2.1190e-04, 1.5912e-04, 9.9804e-01, 3.0016e-04, 5.1444e-04,\n",
       "         1.7116e-04, 8.6053e-05, 8.9181e-05, 2.0549e-04, 6.3765e-05],\n",
       "        [3.2736e-05, 1.7553e-05, 4.6170e-04, 1.7169e-04, 9.9910e-01, 4.0421e-05,\n",
       "         3.9969e-05, 2.7235e-05, 1.7903e-05, 1.3213e-05, 7.3108e-05],\n",
       "        [2.7579e-05, 1.9359e-05, 6.3249e-04, 1.1077e-04, 9.9893e-01, 2.6163e-05,\n",
       "         4.1693e-05, 3.0933e-05, 1.4414e-05, 2.1519e-05, 1.4958e-04],\n",
       "        [3.2212e-05, 1.9937e-05, 7.7785e-04, 8.1562e-05, 9.9872e-01, 2.8533e-05,\n",
       "         5.0583e-05, 3.1145e-05, 1.7425e-05, 2.2687e-05, 2.2072e-04],\n",
       "        [3.1963e-05, 1.8113e-05, 6.9847e-04, 6.4996e-05, 9.9886e-01, 2.4579e-05,\n",
       "         4.4365e-05, 2.7199e-05, 1.9172e-05, 1.9284e-05, 1.9043e-04],\n",
       "        [3.1583e-05, 1.6206e-05, 4.9086e-04, 6.5389e-05, 9.9906e-01, 2.3518e-05,\n",
       "         3.5329e-05, 2.7469e-05, 1.6868e-05, 1.8999e-05, 2.0932e-04],\n",
       "        [3.6707e-05, 1.5634e-05, 4.6400e-04, 4.9477e-05, 9.9907e-01, 2.0363e-05,\n",
       "         3.1231e-05, 2.6525e-05, 1.5182e-05, 2.0661e-05, 2.4647e-04],\n",
       "        [9.9869e-01, 3.3151e-04, 7.3057e-05, 8.8142e-05, 2.8239e-04, 3.8479e-05,\n",
       "         2.6727e-05, 3.1314e-05, 7.5957e-05, 5.8559e-05, 3.0193e-04],\n",
       "        [9.9839e-01, 2.3304e-04, 8.8107e-05, 1.4071e-04, 2.0640e-04, 2.7004e-05,\n",
       "         3.1534e-05, 4.7343e-05, 9.5933e-05, 1.7790e-04, 5.6448e-04],\n",
       "        [4.4063e-05, 1.3920e-04, 4.4411e-05, 1.1624e-04, 2.8337e-05, 4.3600e-05,\n",
       "         3.3254e-05, 1.2284e-04, 9.7554e-05, 9.9916e-01, 1.7252e-04],\n",
       "        [4.7958e-05, 5.3363e-05, 1.4943e-04, 1.9883e-05, 6.0300e-05, 1.5605e-05,\n",
       "         3.4475e-05, 2.8385e-05, 4.6014e-05, 8.8251e-05, 9.9946e-01],\n",
       "        [6.5953e-05, 4.6821e-05, 1.2029e-04, 1.8020e-05, 3.3589e-05, 1.2012e-05,\n",
       "         2.4767e-05, 2.2097e-05, 3.9169e-05, 6.2587e-05, 9.9955e-01],\n",
       "        [7.1621e-05, 4.5766e-05, 1.1753e-04, 1.8717e-05, 3.3582e-05, 1.3313e-05,\n",
       "         2.6908e-05, 2.1814e-05, 4.4322e-05, 4.5801e-05, 9.9956e-01],\n",
       "        [7.7236e-05, 5.0325e-05, 1.2949e-04, 1.9932e-05, 3.7494e-05, 1.3734e-05,\n",
       "         2.7322e-05, 2.3508e-05, 5.3204e-05, 8.5237e-05, 9.9948e-01],\n",
       "        [9.8911e-05, 6.3313e-05, 1.5301e-04, 2.2754e-05, 4.5346e-05, 1.4902e-05,\n",
       "         3.1437e-05, 2.3783e-05, 4.0107e-05, 7.5167e-05, 9.9943e-01],\n",
       "        [1.7294e-04, 7.2123e-05, 2.0103e-04, 2.4543e-05, 7.2447e-05, 1.7396e-05,\n",
       "         3.6688e-05, 2.7107e-05, 4.9056e-05, 5.3969e-05, 9.9927e-01],\n",
       "        [9.1791e-01, 9.4117e-03, 1.5912e-03, 1.4747e-03, 6.8101e-04, 5.1008e-04,\n",
       "         1.3074e-03, 5.8250e-04, 5.8109e-04, 8.4105e-04, 6.5105e-02],\n",
       "        [2.0573e-03, 1.6468e-01, 2.7317e-03, 7.9464e-01, 1.4104e-03, 1.3339e-02,\n",
       "         9.1509e-04, 1.1890e-03, 1.0595e-03, 1.7006e-02, 9.7420e-04],\n",
       "        [9.8601e-05, 3.3646e-05, 1.4394e-04, 1.4638e-03, 1.8774e-04, 9.9749e-01,\n",
       "         3.1492e-04, 1.0470e-04, 6.1532e-05, 6.7205e-05, 3.6607e-05],\n",
       "        [1.3155e-04, 1.7585e-04, 1.3079e-04, 1.5783e-04, 4.0326e-04, 1.1658e-03,\n",
       "         1.0720e-04, 9.9717e-01, 9.7723e-05, 3.2880e-04, 1.3483e-04],\n",
       "        [7.2910e-05, 5.2772e-05, 3.6455e-05, 7.4033e-05, 2.8928e-04, 4.4306e-05,\n",
       "         6.1781e-04, 9.5209e-05, 9.9852e-01, 1.2384e-04, 7.8059e-05],\n",
       "        [4.0853e-05, 5.5898e-05, 4.3691e-05, 7.5657e-05, 2.3707e-04, 3.4899e-05,\n",
       "         1.3509e-03, 3.4649e-05, 9.9787e-01, 1.5527e-04, 9.8447e-05],\n",
       "        [8.4460e-05, 7.4353e-05, 3.2125e-05, 6.3340e-05, 1.4699e-04, 5.3275e-05,\n",
       "         4.6619e-04, 2.5434e-05, 9.9888e-01, 1.1845e-04, 5.9003e-05],\n",
       "        [8.1356e-01, 5.1591e-03, 2.4992e-02, 8.4779e-03, 3.5569e-02, 8.0408e-04,\n",
       "         1.1100e-02, 2.1012e-03, 9.6671e-03, 1.7430e-03, 8.6824e-02]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(**encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = outputs.logits\n",
    "print(logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits[0][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_label_classes = logits.argmax(-1) # use something different than argmax maybe?\n",
    "print(predicted_label_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels = [model.config.id2label[id] for id in predicted_label_classes.squeeze().tolist()]\n",
    "print(predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "predictions = torch.nn.functional.softmax(logits, dim=-1)\n",
    "predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0284, grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[0][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for id, label in zip(encoding.input_ids.squeeze().tolist(), predicted_label_classes[0]):\n",
    "  print(tokenizer.decode([id]), label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'predicted_labels' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [27], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mfor\u001b[39;00m \u001b[39mid\u001b[39m, label \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(encoding\u001b[39m.\u001b[39minput_ids\u001b[39m.\u001b[39msqueeze()\u001b[39m.\u001b[39mtolist(), predicted_labels):\n\u001b[0;32m      2\u001b[0m   \u001b[39mprint\u001b[39m(tokenizer\u001b[39m.\u001b[39mdecode([\u001b[39mid\u001b[39m]), label)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'predicted_labels' is not defined"
     ]
    }
   ],
   "source": [
    "for id, label in zip(encoding.input_ids.squeeze().tolist(), predicted_labels):\n",
    "  print(tokenizer.decode([id]), label)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0069e35276b4b16933fc948b1a9f9ae6be664e95860c7c07ee003b238b1460ad"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
