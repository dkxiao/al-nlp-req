{"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["# Example of text classification of rotten-tomatoes using small-text"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7nMsUwnt_mDe"},"outputs":[],"source":["%pip install small-text[transformers]==1.0.0  # use \"small-text\" without \"[transformers]\" if you want to work on the CPU only\n","\n","# additional dependencies for this example\n","%pip install datasets matplotlib seaborn"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Users\\dekai\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]}],"source":["import datasets\n","datasets.logging.set_verbosity_error()\n","\n","# disables the progress bar for notebooks: https://github.com/huggingface/datasets/issues/2651\n","datasets.logging.get_verbosity = lambda: logging.NOTSET"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["from matplotlib import rcParams\n","rcParams.update({'xtick.labelsize': 14, 'ytick.labelsize': 14, 'axes.labelsize': 16})"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["import torch\n","import numpy as np\n","\n","seed = 2022\n","torch.manual_seed(seed)\n","np.random.seed(seed)"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 3/3 [00:00<00:00, 47.30it/s]\n"]},{"name":"stdout","output_type":"stream","text":["First 10 training samples:\n","\n","1   the rock is destined to be the 21st century's new \" conan \" and that he's going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .\n","1   the gorgeously elaborate continuation of \" the lord of the rings \" trilogy is so huge that a column of words cannot adequately describe co-writer/director peter jackson's expanded vision of j . r . r . tolkien's middle-earth .\n","1   effective but too-tepid biopic\n","1   if you sometimes like to go to the movies to have fun , wasabi is a good place to start .\n","1   emerges as something rare , an issue movie that's so honest and keenly observed that it doesn't feel like one .\n","1   the film provides some great insight into the neurotic mindset of all comics -- even those who have reached the absolute top of the game .\n","1   offers that rare combination of entertainment and education .\n","1   perhaps no picture ever made has more literally showed that the road to hell is paved with good intentions .\n","1   steers turns in a snappy screenplay that curls at the edges ; it's so clever you want to hate it . but he somehow pulls it off .\n","1   take care of my cat offers a refreshingly different slice of asian cinema .\n"]}],"source":["import logging\n","\n","raw_dataset = datasets.load_dataset('rotten_tomatoes')\n","num_classes = np.unique(raw_dataset['train']['label']).shape[0]\n","\n","print('First 10 training samples:\\n')\n","for i in range(10):\n","    print(raw_dataset['train']['label'][i], ' ', raw_dataset['train']['text'][i])"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["from transformers import AutoTokenizer\n","\n","transformer_model_name = 'bert-base-uncased'\n","\n","tokenizer = AutoTokenizer.from_pretrained(\n","    transformer_model_name\n",")"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["from small_text.integrations.transformers.datasets import TransformersDataset\n","\n","\n","def get_transformers_dataset(tokenizer, data, labels, max_length=60):\n","\n","    data_out = []\n","\n","    for i, doc in enumerate(data):\n","        encoded_dict = tokenizer.encode_plus(\n","            doc,\n","            add_special_tokens=True,\n","            padding='max_length',\n","            max_length=max_length,\n","            return_attention_mask=True,\n","            return_tensors='pt',\n","            truncation='longest_first'\n","        )\n","\n","        data_out.append((encoded_dict['input_ids'], encoded_dict['attention_mask'], labels[i]))\n","\n","    return TransformersDataset(data_out)\n","\n","\n","train = get_transformers_dataset(tokenizer, raw_dataset['train']['text'], raw_dataset['train']['label'])\n","test = get_transformers_dataset(tokenizer, raw_dataset['test']['text'], raw_dataset['test']['label'])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from small_text.active_learner import PoolBasedActiveLearner\n","\n","from small_text.initialization import random_initialization_balanced\n","from small_text.integrations.transformers import TransformerModelArguments\n","from small_text.integrations.transformers.classifiers.factories import TransformerBasedClassificationFactory\n","from small_text.query_strategies import PredictionEntropy\n","from small_text.integrations.transformers import TransformerModelArguments\n","\n","\n","# simulates an initial labeling to warm-start the active learning process\n","def initialize_active_learner(active_learner, y_train):\n","\n","    indices_initial = random_initialization_balanced(y_train, n_samples=20)\n","    active_learner.initialize_data(indices_initial, y_train[indices_initial])\n","\n","    return indices_initial\n","\n","\n","\n","transformer_model = TransformerModelArguments(transformer_model_name)\n","clf_factory = TransformerBasedClassificationFactory(transformer_model, \n","                                                    num_classes, \n","                                                    kwargs=dict({'device': 'cuda', \n","                                                                 'mini_batch_size': 32,\n","                                                                 'class_weight': 'balanced'\n","                                                                }))\n","query_strategy = PredictionEntropy()\n","\n","active_learner = PoolBasedActiveLearner(clf_factory, query_strategy, train)\n","indices_labeled = initialize_active_learner(active_learner, train.y)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.metrics import accuracy_score\n","\n","\n","num_queries = 10\n","\n","\n","def evaluate(active_learner, train, test):\n","    y_pred = active_learner.classifier.predict(train)\n","    y_pred_test = active_learner.classifier.predict(test)\n","    \n","    test_acc = accuracy_score(y_pred_test, test.y)\n","\n","    print('Train accuracy: {:.2f}'.format(accuracy_score(y_pred, train.y)))\n","    print('Test accuracy: {:.2f}'.format(test_acc))\n","    \n","    return test_acc\n","\n","\n","results = []\n","results.append(evaluate(active_learner, train[indices_labeled], test))\n","\n","    \n","for i in range(num_queries):\n","    # ...where each iteration consists of labelling 20 samples\n","    indices_queried = active_learner.query(num_samples=20)\n","\n","    # Simulate user interaction here. Replace this for real-world usage.\n","    y = train.y[indices_queried]\n","\n","    # Return the labels for the current query to the active learner.\n","    active_learner.update(y)\n","\n","    indices_labeled = np.concatenate([indices_queried, indices_labeled])\n","    \n","    print('---------------')\n","    print(f'Iteration #{i} ({len(indices_labeled)} samples)')\n","    results.append(evaluate(active_learner, train[indices_labeled], test))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["%matplotlib inline\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","fig = plt.figure(figsize=(12, 8))\n","ax = plt.axes()\n","\n","data = np.vstack((np.arange(num_queries+1), np.array(results)))\n","sns.lineplot(x=0, y=1, data=data)\n","\n","plt.xlabel('number of queries', labelpad=15)\n","plt.ylabel('test accuracy', labelpad=25)\n","\n","sns.despine()"]}],"metadata":{"colab":{"collapsed_sections":[],"provenance":[]},"kernelspec":{"display_name":"Python 3.9.13 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"0069e35276b4b16933fc948b1a9f9ae6be664e95860c7c07ee003b238b1460ad"}}},"nbformat":4,"nbformat_minor":0}
